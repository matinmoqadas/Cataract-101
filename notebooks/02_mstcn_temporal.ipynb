{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "matinmo_cataract_101_path = kagglehub.dataset_download('matinmo/cataract-101')\n",
    "\n",
    "print('Data source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T08:05:16.451765Z",
     "iopub.status.busy": "2025-10-26T08:05:16.451457Z",
     "iopub.status.idle": "2025-10-26T08:05:23.914218Z",
     "shell.execute_reply": "2025-10-26T08:05:23.913288Z",
     "shell.execute_reply.started": "2025-10-26T08:05:16.451744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "PyTorch 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# If running on Kaggle/Colab, most deps are preinstalled.\n",
    "# Uncomment to install anything missing.\n",
    "# !pip -q install --upgrade opencv-python-headless==4.10.0.84 timm==1.0.7\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print('Python', sys.version)\n",
    "print('PyTorch', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-26T08:05:37.409368Z",
     "iopub.status.busy": "2025-10-26T08:05:37.409083Z",
     "iopub.status.idle": "2025-10-26T08:05:37.417127Z",
     "shell.execute_reply": "2025-10-26T08:05:37.416278Z",
     "shell.execute_reply.started": "2025-10-26T08:05:37.409346Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(target_fps=12, resize=112, clip_len_sec=2.0, stride_frac=0.5, subset_n=12, max_frames_per_video=None, epochs=5, batch_size=4, num_workers=2, lr=0.0001, weight_decay=0.0001, label_smoothing=0.0, amp=True, grad_accum_steps=1, median_smooth=15)\n"
     ]
    }
   ],
   "source": [
    "# === Set your dataset paths ===\n",
    "# Example (Kaggle): /kaggle/input/cataract-101/cataract-101\n",
    "DATA_ROOT = Path('/kaggle/input/cataract-101/cataract-101')  # <- change if needed\n",
    "ANN_PATH  = DATA_ROOT / 'annotations.csv'                    # <- change if needed\n",
    "\n",
    "# Where to save checkpoints & artifacts\n",
    "WORK_DIR = Path('/kaggle/working/phase2_clip_baseline')\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model/data config\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    target_fps: int = 12            # decode at this FPS for training/inference\n",
    "    resize: int = 112               # r3d_18 was pretrained at 112x112\n",
    "    clip_len_sec: float = 2.0       # ~2 seconds clips\n",
    "    stride_frac: float = 0.5        # 50% overlap during training\n",
    "    subset_n: Optional[int] = 12    # limit number of videos to fit 20GB; None=all\n",
    "    max_frames_per_video: Optional[int] = None  # e.g., 5000 to cap long videos\n",
    "\n",
    "    # Train\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 4\n",
    "    num_workers: int = 2\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    label_smoothing: float = 0.0\n",
    "    amp: bool = True\n",
    "    grad_accum_steps: int = 1\n",
    "\n",
    "    # Evaluation/postproc\n",
    "    median_smooth: int = 15         # odd window size for timeline smoothing\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-10-26T08:05:41.948456Z",
     "iopub.status.busy": "2025-10-26T08:05:41.948146Z",
     "iopub.status.idle": "2025-10-26T08:05:41.975716Z",
     "shell.execute_reply": "2025-10-26T08:05:41.974717Z",
     "shell.execute_reply.started": "2025-10-26T08:05:41.948408Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1233736004.py, line 138)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_37/1233736004.py\"\u001b[0;36m, line \u001b[0;32m138\u001b[0m\n\u001b[0;31m    any(k in c for k in keys if isinstance(keys, (list,tuple)) else [keys]):\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3 — Robust reader for semicolon Cataract-101 annotations ===\n",
    "# Outputs:\n",
    "#   df  : FRAME schema → [video_id, frame_idx, phase_id]\n",
    "#   videos : {video_id -> Path}\n",
    "#   fps_by_vid, num_classes, phase_to_idx, idx_to_phase\n",
    "\n",
    "import re, math\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import cv2\n",
    "\n",
    "assert 'DATA_ROOT' in globals(), \"Run Cell 2 first to set DATA_ROOT.\"\n",
    "DATA_ROOT = Path(DATA_ROOT)\n",
    "\n",
    "ANN_PATH   = DATA_ROOT / 'annotations.csv'\n",
    "VIDEOS_CSV = DATA_ROOT / 'videos.csv'\n",
    "PHASES_CSV = DATA_ROOT / 'phases.csv'\n",
    "VIDEO_DIR  = DATA_ROOT / 'videos'\n",
    "if not ANN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"annotations.csv not found at {ANN_PATH}\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).strip().lower().replace('-', '_').replace(' ', '_')\n",
    "    return re.sub(r'[^a-z0-9_]', '', s)\n",
    "\n",
    "def find_col(df_or_cols, aliases):\n",
    "    cols = list(df_or_cols.columns) if hasattr(df_or_cols, \"columns\") else list(df_or_cols)\n",
    "    if not cols: return None\n",
    "    for a in aliases:\n",
    "        if a in cols: return a\n",
    "    nmap = {_norm(c): c for c in cols}\n",
    "    for a in aliases:\n",
    "        na = _norm(a)\n",
    "        if na in nmap: return nmap[na]\n",
    "    low = {_norm(c): c for c in cols}\n",
    "    for a in aliases:\n",
    "        na = _norm(a)\n",
    "        for lc, orig in low.items():\n",
    "            if na in lc: return orig\n",
    "    return None\n",
    "\n",
    "# ---------- videos & fps ----------\n",
    "VIDEO_EXTS = {'.mp4', '.mov', '.avi', '.mkv'}\n",
    "videos = {}\n",
    "if VIDEO_DIR.exists():\n",
    "    for p in VIDEO_DIR.rglob('*'):\n",
    "        if p.suffix.lower() in VIDEO_EXTS:\n",
    "            videos[p.stem] = p\n",
    "print(f\"[info] found {len(videos)} video files on disk\")\n",
    "\n",
    "def get_video_fps(path: Path) -> float:\n",
    "    cap = cv2.VideoCapture(str(path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    cap.release()\n",
    "    return float(fps) if fps and fps > 1e-3 else 25.0\n",
    "\n",
    "fps_by_vid = {}\n",
    "if VIDEOS_CSV.exists():\n",
    "    vdf = pd.read_csv(VIDEOS_CSV)\n",
    "    vdf.columns = [_norm(c) for c in vdf.columns]\n",
    "    vcol = find_col(vdf, ['video_id','video','vid','case_id','filename','file','name'])\n",
    "    fcol = find_col(vdf, ['fps','frame_rate','framerate'])\n",
    "    if vcol:\n",
    "        vdf['__stem__'] = vdf[vcol].astype(str).str.replace(r'\\.[^.]+$', '', regex=True)\n",
    "        if fcol:\n",
    "            for r in vdf[['__stem__', fcol]].itertuples(index=False):\n",
    "                try: fps_by_vid[str(r[0])] = float(r[1])\n",
    "                except: pass\n",
    "        # ensure stem->file\n",
    "        for stem in vdf['__stem__'].unique():\n",
    "            if stem not in videos:\n",
    "                for p in VIDEO_DIR.rglob('*'):\n",
    "                    if p.suffix.lower() in VIDEO_EXTS and p.stem == str(stem):\n",
    "                        videos[str(stem)] = p\n",
    "\n",
    "# ---------- read annotations (force semicolon) ----------\n",
    "# Try normal; if a single column or header contains ';', re-read with sep=';'\n",
    "ann = pd.read_csv(ANN_PATH, engine='python', sep=None)\n",
    "needs_semicolon = (ann.shape[1] == 1) or any(';' in str(c) for c in ann.columns)\n",
    "if needs_semicolon:\n",
    "    # Try with header; if that still yields 1 column, use header=None then split\n",
    "    ann_try = pd.read_csv(ANN_PATH, sep=';', engine='python', on_bad_lines='warn')\n",
    "    if ann_try.shape[1] == 1:\n",
    "        ann = pd.read_csv(ANN_PATH, sep=';', engine='python', header=None, names=['__one__'])\n",
    "        # Split the single column into 3 fields \"video;frame;phase\"\n",
    "        split = ann['__one__'].astype(str).str.split(';', n=2, expand=True)\n",
    "        if split.shape[1] != 3:\n",
    "            raise ValueError(\"Expected 'video;frame;phase' format when splitting semicolon annotations.\")\n",
    "        ann = split\n",
    "        ann.columns = ['video', 'frame', 'phase']\n",
    "    else:\n",
    "        ann = ann_try\n",
    "\n",
    "# normalize column names\n",
    "ann.columns = [_norm(c) for c in ann.columns]\n",
    "cols = set(ann.columns)\n",
    "print(\"[info] annotations columns (normalized):\", sorted(cols))\n",
    "\n",
    "# If we *still* have 1 column, split it here defensively\n",
    "if len(ann.columns) == 1:\n",
    "    only = ann.columns[0]\n",
    "    if ann[only].astype(str).str.contains(';').any():\n",
    "        split = ann[only].astype(str).str.split(';', n=2, expand=True)\n",
    "        if split.shape[1] == 3:\n",
    "            ann = split\n",
    "            ann.columns = ['video','frame','phase']\n",
    "            ann.columns = [_norm(c) for c in ann.columns]\n",
    "\n",
    "# Detect canonical col names now\n",
    "col_video = find_col(ann, ['video','video_id','vid','case_id','filename','file','name'])\n",
    "col_frame = find_col(ann, ['frame','frame_idx','frame_id','frameindex','fid'])\n",
    "col_phase = find_col(ann, ['phase','label','phase_id','class','action','phase_name'])\n",
    "\n",
    "if not (col_video and col_frame and col_phase):\n",
    "    raise ValueError(f\"Could not find video/frame/phase columns in annotations. Got: {list(ann.columns)}\")\n",
    "\n",
    "# Normalize video ids → stems; then map 269 → case_269\n",
    "ann[col_video] = ann[col_video].astype(str).str.replace(r'\\.[^.]+$', '', regex=True)\n",
    "\n",
    "def _to_case_stem(v):\n",
    "    s = str(v).strip()\n",
    "    if s.startswith('case_'): return s\n",
    "    # if value is something like '269' or 'case269'\n",
    "    m = re.search(r'(\\d+)$', s)\n",
    "    return f\"case_{m.group(1)}\" if m else s\n",
    "\n",
    "# Build the FRAME schema df\n",
    "tmp = ann[[col_video, col_frame, col_phase]].copy()\n",
    "tmp.columns = ['video_id', 'frame_idx', 'phase_raw']\n",
    "tmp['video_id'] = tmp['video_id'].map(_to_case_stem)\n",
    "\n",
    "# phase mapping\n",
    "if pd.api.types.is_numeric_dtype(tmp['phase_raw']):\n",
    "    tmp['phase_id_raw'] = pd.to_numeric(tmp['phase_raw'], errors='coerce').astype('Int64').fillna(-1).astype(int)\n",
    "else:\n",
    "    cats = pd.Categorical(tmp['phase_raw'])\n",
    "    tmp['phase_id_raw'] = cats.codes.astype(int)\n",
    "    print(\"[info] phase name→id mapping (first 10):\", {int(i): l for i,l in list(enumerate(cats.categories))[:10]})\n",
    "\n",
    "uniq_raw = pd.Index(sorted(tmp['phase_id_raw'].dropna().unique()))\n",
    "phase_to_idx = {int(p): i for i,p in enumerate(uniq_raw)}\n",
    "idx_to_phase = {i:int(p) for i,p in enumerate(uniq_raw)}\n",
    "num_classes = len(uniq_raw)\n",
    "tmp['phase_id'] = tmp['phase_id_raw'].map(phase_to_idx).astype(int)\n",
    "\n",
    "# frame_idx as int\n",
    "tmp['frame_idx'] = pd.to_numeric(tmp['frame_idx'], errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    "# final df\n",
    "df = tmp[['video_id','frame_idx','phase_id']].sort_values(['video_id','frame_idx']).reset_index(drop=True)\n",
    "\n",
    "# sanity\n",
    "print(f\"[info] Detected FRAME-LEVEL schema with columns: {df.columns.tolist()}\")\n",
    "print(f\"[info] Num phases = {num_classes}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Map annotation video_ids like \"934\" → \"case_934\"\n",
    "assert 'df' in globals(), \"Run Cell 3 first.\"\n",
    "\n",
    "def _to_case_stem(v):\n",
    "    s = str(v).strip()\n",
    "    # already good\n",
    "    if s.startswith('case_'):\n",
    "        return s\n",
    "    # strip any extension just in case\n",
    "    s = re.sub(r'\\.[^.]+$', '', s)\n",
    "    # keep only trailing digits\n",
    "    m = re.search(r'(\\d+)$', s)\n",
    "    if m:\n",
    "        return f\"case_{m.group(1)}\"\n",
    "    return s  # fallback\n",
    "\n",
    "df['video_id'] = df['video_id'].map(_to_case_stem)\n",
    "\n",
    "# Re-check which ids match files we discovered\n",
    "missing = sorted(set(df['video_id']) - set(videos.keys()))\n",
    "print(f\"[alias] after mapping, unmatched annotated videos: {len(missing)}\")\n",
    "print(missing[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check unmatched video ids after mapping to case_{id}\n",
    "missing = sorted(set(df['video_id']) - set(videos.keys()))\n",
    "print(f\"[check] unmatched after mapping: {len(missing)}\")\n",
    "print(missing[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Patched Cell — Robust timeline builder (no double phase mapping) ===\n",
    "from typing import Dict, Optional\n",
    "import numpy as np, math\n",
    "from pathlib import Path\n",
    "\n",
    "def _encode_phase(val, num_classes: Optional[int]=None, phase_to_idx: Optional[Dict[int,int]]=None) -> int:\n",
    "    try:\n",
    "        v = int(val)\n",
    "    except Exception:\n",
    "        return 0\n",
    "    if num_classes is not None and 0 <= v < num_classes:\n",
    "        return v\n",
    "    if phase_to_idx and v in phase_to_idx:\n",
    "        return int(phase_to_idx[v])\n",
    "    if num_classes is not None and num_classes > 0:\n",
    "        return int(v) % num_classes\n",
    "    return int(max(v, 0))\n",
    "\n",
    "def build_timelines(df_ann: pd.DataFrame,\n",
    "                    videos: Dict[str, Path],\n",
    "                    target_fps: int,\n",
    "                    max_frames_per_video: Optional[int]=None,\n",
    "                    num_classes: Optional[int]=None,\n",
    "                    phase_to_idx: Optional[Dict[int,int]]=None) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Returns: dict[video_id] = {fps: int, n_frames: int, labels: np.ndarray[int]}\n",
    "    Works without actual video files; assumes fps=25 if unknown.\n",
    "    \"\"\"\n",
    "    def default_fps_for(vid: str) -> float:\n",
    "        if isinstance(videos, dict) and vid in videos and callable(globals().get(\"get_video_fps\", None)):\n",
    "            try:\n",
    "                return float(max(1e-3, globals()[\"get_video_fps\"](videos[vid])))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return 25.0\n",
    "\n",
    "    if num_classes is None and \"phase_id\" in df_ann.columns:\n",
    "        try:\n",
    "            num_classes = int(df_ann[\"phase_id\"].max()) + 1\n",
    "        except Exception:\n",
    "            num_classes = None\n",
    "\n",
    "    timelines: Dict[str, Dict] = {}\n",
    "\n",
    "    # SEGMENT schema\n",
    "    if set([\"video_id\",\"start_sec\",\"end_sec\",\"phase_id\"]).issubset(df_ann.columns):\n",
    "        by_vid = df_ann.groupby(\"video_id\", sort=False)\n",
    "        for vid, g in by_vid:\n",
    "            total_sec = float(max(g[\"end_sec\"].values) if len(g) else 0.0)\n",
    "            n_tgt = int(max(1, math.ceil(total_sec * target_fps)))\n",
    "            labels = np.full(n_tgt, fill_value=0, dtype=np.int64)\n",
    "            for r in g.itertuples(index=False):\n",
    "                p = _encode_phase(getattr(r, \"phase_id\"), num_classes, phase_to_idx)\n",
    "                a = max(0, int(round(float(getattr(r, \"start_sec\")) * target_fps)))\n",
    "                b = min(n_tgt, int(round(float(getattr(r, \"end_sec\"))   * target_fps)))\n",
    "                if b <= a:\n",
    "                    b = min(n_tgt, a + 1)\n",
    "                labels[a:b] = p\n",
    "            if max_frames_per_video is not None:\n",
    "                labels = labels[:max_frames_per_video]\n",
    "            timelines[vid] = dict(fps=target_fps, n_frames=int(labels.shape[0]), labels=labels)\n",
    "\n",
    "    # FRAME schema\n",
    "    elif set([\"video_id\",\"frame_idx\",\"phase_id\"]).issubset(df_ann.columns):\n",
    "        by_vid = df_ann.groupby(\"video_id\", sort=False)\n",
    "        for vid, g in by_vid:\n",
    "            fps_src = default_fps_for(vid)\n",
    "            max_src_idx = int(g[\"frame_idx\"].max()) + 1 if len(g) else 1\n",
    "            total_sec = max_src_idx / max(fps_src, 1e-6)\n",
    "            n_tgt = int(max(1, math.ceil(total_sec * target_fps)))\n",
    "            labels = np.full(n_tgt, fill_value=0, dtype=np.int64)\n",
    "            for r in g.itertuples(index=False):\n",
    "                p  = _encode_phase(getattr(r, \"phase_id\"), num_classes, phase_to_idx)\n",
    "                fi = int(getattr(r, \"frame_idx\"))\n",
    "                t  = int(round((fi / max(fps_src, 1e-6)) * target_fps))\n",
    "                t  = int(np.clip(t, 0, n_tgt - 1))\n",
    "                labels[t] = p\n",
    "            if max_frames_per_video is not None:\n",
    "                labels = labels[:max_frames_per_video]\n",
    "            timelines[vid] = dict(fps=target_fps, n_frames=int(labels.shape[0]), labels=labels)\n",
    "    else:\n",
    "        raise ValueError(\"df must be SEGMENT (video_id,start_sec,end_sec,phase_id) or FRAME (video_id,frame_idx,phase_id) schema.\")\n",
    "\n",
    "    print('Built timelines for', len(timelines), 'videos')\n",
    "    return timelines\n",
    "\n",
    "# Rebuild with robust function\n",
    "TIMELINES = build_timelines(\n",
    "    df,\n",
    "    videos,\n",
    "    target_fps=CFG.target_fps,\n",
    "    max_frames_per_video=CFG.max_frames_per_video if hasattr(CFG, \"max_frames_per_video\") else None,\n",
    "    num_classes=(num_classes if \"num_classes\" in globals() else None),\n",
    "    phase_to_idx=(phase_to_idx if \"phase_to_idx\" in globals() else None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_clip_index(timelines: Dict[str, Dict], clip_len_sec: float, stride_frac: float) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for vid, info in timelines.items():\n",
    "        labels = info['labels']\n",
    "        fps = info['fps']\n",
    "        L = int(round(clip_len_sec * fps))\n",
    "        S = max(1, int(round(L * stride_frac)))\n",
    "        if L < 1: \n",
    "            continue\n",
    "        for a in range(0, len(labels)-L+1, S):\n",
    "            b = a + L\n",
    "            window = labels[a:b]\n",
    "            lab = window[L//2]  # center frame label\n",
    "            rows.append((vid, a, b, int(lab)))\n",
    "    idx = pd.DataFrame(rows, columns=['video_id','start','end','label'])\n",
    "    return idx\n",
    "\n",
    "CLIP_INDEX = build_clip_index(TIMELINES, CFG.clip_len_sec, CFG.stride_frac)\n",
    "print('Total clips:', len(CLIP_INDEX))\n",
    "\n",
    "# Train/val split by video (no leakage)\n",
    "all_vids = list(TIMELINES.keys())\n",
    "random.shuffle(all_vids)\n",
    "split = int(0.8*len(all_vids)) if len(all_vids) > 1 else 1\n",
    "train_vids, val_vids = set(all_vids[:split]), set(all_vids[split:])\n",
    "\n",
    "def split_by_videos(index_df: pd.DataFrame, train_set: set, val_set: set):\n",
    "    tr = index_df[index_df.video_id.isin(train_set)].reset_index(drop=True)\n",
    "    va = index_df[index_df.video_id.isin(val_set)].reset_index(drop=True)\n",
    "    return tr, va\n",
    "\n",
    "CLIPS_TR, CLIPS_VA = split_by_videos(CLIP_INDEX, train_vids, val_vids)\n",
    "print(CLIPS_TR.shape, CLIPS_VA.shape)\n",
    "\n",
    "# Class weights from training labels (for imbalance)\n",
    "cnt = Counter(CLIPS_TR['label'].tolist())\n",
    "class_counts = np.array([cnt.get(i,0) for i in range(num_classes)])\n",
    "class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
    "class_weights = class_weights / class_weights.sum() * num_classes\n",
    "print('Class counts:', class_counts)\n",
    "print('Class weights:', class_weights.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, clips_df: pd.DataFrame, timelines: Dict[str, Dict], video_paths: Dict[str, Path],\n",
    "                 resize: int, augment: bool, target_fps: int):\n",
    "        self.df = clips_df\n",
    "        self.timelines = timelines\n",
    "        self.videos = video_paths\n",
    "        self.size = resize\n",
    "        self.augment = augment\n",
    "        self.fps = target_fps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _read_window(self, path: Path, a: int, b: int) -> np.ndarray:\n",
    "        # Read frames for [a, b) at self.fps using nearest timestamp mapping\n",
    "        cap = cv2.VideoCapture(str(path))\n",
    "        fps_src = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "        frames = []\n",
    "        last_frame = None\n",
    "        for t in range(a, b):\n",
    "            src_idx = int(round((t / self.fps) * fps_src))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, src_idx)\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                frame = last_frame if last_frame is not None else np.zeros((self.size, self.size, 3), np.uint8)\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "                last_frame = frame\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        arr = np.stack(frames, axis=0)  # [T,H,W,3]\n",
    "        return arr\n",
    "\n",
    "    def _augment(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Simple spatial augs (random horizontal flip)\n",
    "        if not self.augment:\n",
    "            return x\n",
    "        if random.random() < 0.5:\n",
    "            x = x[:, :, ::-1, :]\n",
    "        return x\n",
    "\n",
    "    def _normalize(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = x.astype(np.float32) / 255.0\n",
    "        x = (x - IMAGENET_MEAN) / IMAGENET_STD\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        vid, a, b, lab = r.video_id, int(r.start), int(r.end), int(r.label)\n",
    "        path = self.videos[vid]\n",
    "        x = self._read_window(path, a, b)   # [T,H,W,3]\n",
    "        x = self._augment(x)\n",
    "        x = self._normalize(x)\n",
    "        x = torch.from_numpy(x).permute(3,0,1,2)  # [3,T,H,W]\n",
    "        y = torch.tensor(lab, dtype=torch.long)\n",
    "        return x, y, vid, a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = ClipDataset(CLIPS_TR, TIMELINES, videos, resize=CFG.resize, augment=True,  target_fps=CFG.target_fps)\n",
    "val_ds   = ClipDataset(CLIPS_VA, TIMELINES, videos, resize=CFG.resize, augment=False, target_fps=CFG.target_fps)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,  num_workers=CFG.num_workers, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "\n",
    "print('Train clips:', len(train_ds), ' Val clips:', len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pretrained backbone\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "net = r3d_18(weights=weights)\n",
    "net.fc = nn.Linear(net.fc.in_features, num_classes)\n",
    "\n",
    "# Optional: freeze early layers for speed on small data\n",
    "# for name, p in net.named_parameters():\n",
    "#     if not name.startswith('layer4') and 'fc' not in name:\n",
    "#         p.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(CFG.epochs,1))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, target):\n",
    "    return (logits.argmax(1) == target).float().mean().item()\n",
    "\n",
    "best_acc = 0.0\n",
    "BEST_PATH = WORK_DIR / 'best_clip_model.pt'\n",
    "\n",
    "for epoch in range(1, CFG.epochs+1):\n",
    "    net.train()\n",
    "    t0 = time.time()\n",
    "    tr_loss = 0.0\n",
    "    tr_acc = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (x,y,_,_,_) in enumerate(train_dl, 1):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.amp):\n",
    "            logits = net(x)\n",
    "            loss = F.cross_entropy(logits, y, label_smoothing=CFG.label_smoothing, reduction='mean')\n",
    "        scaler.scale(loss/CFG.grad_accum_steps).backward()\n",
    "        if step % CFG.grad_accum_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        tr_loss += loss.item()\n",
    "        tr_acc  += accuracy(logits.detach(), y)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    va_loss = 0.0\n",
    "    va_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x,y,_,_,_ in val_dl:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.amp):\n",
    "                logits = net(x)\n",
    "                loss = F.cross_entropy(logits, y, label_smoothing=0.0, reduction='mean')\n",
    "            va_loss += loss.item()\n",
    "            va_acc  += accuracy(logits, y)\n",
    "\n",
    "    ntr = max(1, len(train_dl)); nva = max(1, len(val_dl))\n",
    "    tr_loss/=ntr; tr_acc/=ntr; va_loss/=nva; va_acc/=nva\n",
    "    dt = time.time()-t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | {dt:5.1f}s | \"\n",
    "          f\"train loss {tr_loss:.3f} acc {tr_acc:.3f} | val loss {va_loss:.3f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        torch.save({'model': net.state_dict(),\n",
    "                    'num_classes': num_classes,\n",
    "                    'phase_to_idx': phase_to_idx}, BEST_PATH)\n",
    "        print('  -> saved best to', BEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "all_rows = []\n",
    "with torch.no_grad():\n",
    "    for x,y,vid,a,b in val_dl:\n",
    "        x = x.to(device)\n",
    "        logits = net(x)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        pred = probs.argmax(axis=1)\n",
    "        for i in range(len(vid)):\n",
    "            all_rows.append((vid[i], int(a[i]), int(b[i]), int(pred[i])))\n",
    "\n",
    "VAL_PREDS = pd.DataFrame(all_rows, columns=['video_id','start','end','pred'])\n",
    "print(VAL_PREDS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "def stitch_predictions(pred_df: pd.DataFrame, timelines: Dict[str, Dict], num_classes: int,\n",
    "                       smooth: int = 15) -> Dict[str, np.ndarray]:\n",
    "    pred_timelines = {}\n",
    "    for vid, g in pred_df.groupby('video_id'):\n",
    "        if vid not in timelines:\n",
    "            continue\n",
    "        n = timelines[vid]['n_frames']\n",
    "        votes = np.zeros((num_classes, n), dtype=np.int32)\n",
    "        for r in g.itertuples(index=False):\n",
    "            votes[r.pred, r.start:r.end] += 1\n",
    "        out = votes.argmax(axis=0)\n",
    "        if smooth and smooth % 2 == 1 and smooth > 1:\n",
    "            out = medfilt(out, kernel_size=smooth)\n",
    "        pred_timelines[vid] = out\n",
    "    return pred_timelines\n",
    "\n",
    "PRED_TIMELINES = stitch_predictions(VAL_PREDS, TIMELINES, num_classes, smooth=CFG.median_smooth)\n",
    "print('Pred timelines for', len(PRED_TIMELINES), 'videos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def eval_frame_metrics(pred_tl: Dict[str, np.ndarray], gt: Dict[str, Dict]) -> Dict[str, float]:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for vid, pred in pred_tl.items():\n",
    "        if vid not in gt:\n",
    "            continue\n",
    "        ref = gt[vid]['labels']\n",
    "        m = min(len(ref), len(pred))\n",
    "        y_true.append(ref[:m])\n",
    "        y_pred.append(pred[:m])\n",
    "    if not y_true:\n",
    "        return dict(frame_acc=float('nan'), macro_f1=float('nan'))\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = (y_true == y_pred).mean().item()\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return dict(frame_acc=acc, macro_f1=macro_f1)\n",
    "\n",
    "metrics = eval_frame_metrics(PRED_TIMELINES, TIMELINES)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot ground truth vs. predicted for one validation video\n",
    "val_example_vid = next(iter(set(CLIPS_VA.video_id))) if len(CLIPS_VA) else None\n",
    "if val_example_vid is not None and val_example_vid in PRED_TIMELINES:\n",
    "    y_gt = TIMELINES[val_example_vid]['labels']\n",
    "    y_pr = PRED_TIMELINES[val_example_vid]\n",
    "    T = min(len(y_gt), len(y_pr))\n",
    "    t = np.arange(T) / CFG.target_fps\n",
    "\n",
    "    plt.figure(figsize=(14,3))\n",
    "    plt.plot(t, y_gt[:T], lw=2, label='GT', alpha=0.9)\n",
    "    plt.plot(t, y_pr[:T], lw=1, label='Pred', alpha=0.9)\n",
    "    plt.xlabel('Seconds')\n",
    "    plt.ylabel('Phase idx')\n",
    "    plt.title(f'Video: {val_example_vid} — GT vs Predicted (smoothed={CFG.median_smooth})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No validation example available to visualize.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best model (optional; already loaded during training)\n",
    "ckpt = torch.load(WORK_DIR / 'best_clip_model.pt', map_location='cpu')\n",
    "net.load_state_dict(ckpt['model'])\n",
    "net.eval()\n",
    "print('Loaded best model with classes:', ckpt['num_classes'])\n",
    "\n",
    "# Export per-frame predictions for all VAL videos (CSV)\n",
    "rows = []\n",
    "for vid, arr in PRED_TIMELINES.items():\n",
    "    for i, p in enumerate(arr):\n",
    "        rows.append((vid, i, p))\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=['video_id','t_idx','pred_label_idx'])\n",
    "CSV_OUT = WORK_DIR / 'val_frame_predictions.csv'\n",
    "df_out.to_csv(CSV_OUT, index=False)\n",
    "print('Saved:', CSV_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enforce a minimum duration per segment to reduce rapid label jitter.\n",
    "def apply_min_duration(arr: np.ndarray, min_len_frames: int) -> np.ndarray:\n",
    "    x = arr.copy()\n",
    "    i = 0\n",
    "    n = len(x)\n",
    "    while i < n:\n",
    "        j = i\n",
    "        while j < n and x[j] == x[i]:\n",
    "            j += 1\n",
    "        run_len = j - i\n",
    "        if run_len < min_len_frames:\n",
    "            left = x[i-1] if i-1 >= 0 else x[j if j < n else i]\n",
    "            right = x[j] if j < n else x[i-1]\n",
    "            fill = left if left == right else (left if i>0 else right)\n",
    "            x[i:j] = fill\n",
    "        i = j\n",
    "    return x\n",
    "\n",
    "# Example usage on one video\n",
    "val_example_vid = next(iter(PRED_TIMELINES.keys())) if PRED_TIMELINES else None\n",
    "if val_example_vid is not None:\n",
    "    pred0 = PRED_TIMELINES[val_example_vid]\n",
    "    pred1 = apply_min_duration(pred0, min_len_frames=int(1.0*CFG.target_fps))  # 1s minimum\n",
    "    print('Changed frames:', int((pred0!=pred1).sum()))\n",
    "else:\n",
    "    print('No predictions to post-process.')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8540918,
     "sourceId": 13455357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
