{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1c8d98",
   "metadata": {},
   "source": [
    "# MS-TCN Prototype Training Notebook\n",
    "\n",
    "This notebook implements a minimal MS-TCN-style training pipeline for a quick prototype on the `cataract-101` dataset.\n",
    "\n",
    "It expects per-video pre-extracted features at `cataract-101/features/{video_id}.npy` and a `cataract-101/segments_filled.csv` file.\n",
    "\n",
    "**What this notebook contains**:\n",
    "- Configuration cell you can edit\n",
    "- Dataset loader for fixed-length clips from pre-extracted features\n",
    "- Lightweight single-stage MS-TCN model\n",
    "- Training loop (run for a short smoke test)\n",
    "- Single-video inference example\n",
    "\n",
    "Run cells sequentially. The notebook **does not** run full training automatically; you control when to start training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34b3e0",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install required packages if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision numpy pandas scikit-learn pyyaml opencv-python tqdm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "cfg = {\n",
    "    \"data\": {\n",
    "        \"dataset_root\": \"cataract-101\",\n",
    "        \"features_dir\": \"features\",\n",
    "        \"segments_csv\": \"segments_filled.csv\",\n",
    "        \"sample_size\": 20,      # number of videos to use for quick prototype (set None to use all)\n",
    "        \"seq_len\": 200,\n",
    "        \"fps\": 25\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"feat_dim\": 512,       # set to your feature dim\n",
    "        \"hidden_dim\": 64,\n",
    "        \"num_layers\": 6,\n",
    "        \"num_classes\": 10      # overwrite with true number of phases\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 3,           # small number for smoke test\n",
    "        \"batch_size\": 2,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"device\": \"cuda\" if __import__('torch').cuda.is_available() else \"cpu\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "    \"io\": {\n",
    "        \"out_dir\": \"exp_ms_tcn_sample\",\n",
    "        \"checkpoint_freq\": 1\n",
    "    }\n",
    "}\n",
    "print('Config ready - please edit values above to match your setup.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec33711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, random, yaml, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "print('imports ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7381cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (expects features/{video_id}.npy and segments_filled.csv)\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, cfg, split='train'):\n",
    "        self.root = cfg['data']['dataset_root']\n",
    "        self.features_dir = os.path.join(self.root, cfg['data']['features_dir'])\n",
    "        self.seq_len = cfg['data']['seq_len']\n",
    "        self.sample_size = cfg['data'].get('sample_size', None)\n",
    "        self.segments_csv = os.path.join(self.root, cfg['data']['segments_csv'])\n",
    "        self.split = split\n",
    "\n",
    "        seg_df = pd.read_csv(self.segments_csv)\n",
    "        # build per-video label arrays\n",
    "        self.video_labels = {}\n",
    "        for vid, g in seg_df.groupby('video_id'):\n",
    "            max_frame = int(g['end_frame'].max()) + 1\n",
    "            labels = np.zeros((max_frame,), dtype=np.int64) - 1\n",
    "            for _, row in g.iterrows():\n",
    "                s = int(row['start_frame'])\n",
    "                e = int(row['end_frame'])\n",
    "                labels[s:e+1] = int(row['phase_id'])\n",
    "            if labels.max() >= 0:\n",
    "                self.video_labels[vid] = labels\n",
    "\n",
    "        self.videos = list(self.video_labels.keys())\n",
    "        if self.sample_size is not None:\n",
    "            random.seed(cfg['training']['seed'])\n",
    "            random.shuffle(self.videos)\n",
    "            self.videos = self.videos[: self.sample_size]\n",
    "\n",
    "        # map video->feature path (may be missing)\n",
    "        self.video_feats = {}\n",
    "        for vid in self.videos:\n",
    "            feat_path = os.path.join(self.features_dir, f\"{vid}.npy\")\n",
    "            self.video_feats[vid] = feat_path if os.path.exists(feat_path) else None\n",
    "\n",
    "        # build index entries of windows\n",
    "        self.index = []\n",
    "        for vid in self.videos:\n",
    "            n = len(self.video_labels[vid])\n",
    "            if n < self.seq_len:\n",
    "                continue\n",
    "            for s in range(0, n - self.seq_len + 1, self.seq_len):\n",
    "                self.index.append((vid, s))\n",
    "        if len(self.index) == 0:\n",
    "            for vid in self.videos:\n",
    "                n = len(self.video_labels[vid])\n",
    "                if n >= self.seq_len:\n",
    "                    for s in range(0, n - self.seq_len + 1, max(1, self.seq_len // 2)):\n",
    "                        self.index.append((vid, s))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid, s = self.index[idx]\n",
    "        feat_path = self.video_feats[vid]\n",
    "        labels = self.video_labels[vid]\n",
    "        if feat_path is None:\n",
    "            raise FileNotFoundError(f\"Missing features for video {vid}. Expected {feat_path}\")\n",
    "        feats = np.load(feat_path)\n",
    "        clip_feats = feats[s : s + self.seq_len]\n",
    "        clip_lbls = labels[s : s + self.seq_len]\n",
    "        return {\n",
    "            'video_id': vid,\n",
    "            'start': s,\n",
    "            'feats': torch.from_numpy(clip_feats).float(),\n",
    "            'labels': torch.from_numpy(clip_lbls).long()\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = torch.stack([b['feats'] for b in batch], dim=0)\n",
    "    labels = torch.stack([b['labels'] for b in batch], dim=0)\n",
    "    return {'feats': feats, 'labels': labels, 'video_id': [b['video_id'] for b in batch], 'start': [b['start'] for b in batch]}\n",
    "\n",
    "print('Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple single-stage MS-TCN-like model\n",
    "class DilatedResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__()\n",
    "        self.conv_dilated = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_dilated(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv_1x1(out)\n",
    "        out = self.dropout(out)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return self.relu(out + x)\n",
    "\n",
    "class SimpleMS_TCN(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(feat_dim, hidden_dim, kernel_size=1)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** (i % 4)\n",
    "            layers.append(DilatedResidualLayer(hidden_dim, hidden_dim, dilation=dilation))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Conv1d(hidden_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats: (B, T, D)\n",
    "        x = feats.permute(0, 2, 1)  # -> (B, D, T)\n",
    "        x = self.input_conv(x)\n",
    "        x = self.tcn(x)\n",
    "        out = self.classifier(x)\n",
    "        out = out.permute(0, 2, 1)  # (B, T, C)\n",
    "        return out\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics helpers\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_frame_metrics(gt, pred, num_classes):\n",
    "    mask = gt != -1\n",
    "    gt = gt[mask]; pred = pred[mask]\n",
    "    acc = accuracy_score(gt, pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(gt, pred, labels=list(range(num_classes)), zero_division=0)\n",
    "    macro_f1 = float(np.nanmean(f1))\n",
    "    per_class = {f'p_{i}': float(p[i]) for i in range(len(p))}\n",
    "    per_class.update({f'r_{i}': float(r[i]) for i in range(len(r))})\n",
    "    per_class.update({f'f1_{i}': float(f1[i]) for i in range(len(f1))})\n",
    "    res = {'acc': float(acc), 'macro_f1': macro_f1}\n",
    "    res.update(per_class)\n",
    "    return res\n",
    "\n",
    "def collapse_sequence(x):\n",
    "    out = []\n",
    "    for v in x:\n",
    "        if v == -1:\n",
    "            continue\n",
    "        if len(out)==0 or out[-1] != v:\n",
    "            out.append(int(v))\n",
    "    return out\n",
    "\n",
    "def levenshtein(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    dp = np.zeros((n+1, m+1), dtype=int)\n",
    "    for i in range(1, n+1): dp[i,0] = i\n",
    "    for j in range(1, m+1): dp[0,j] = j\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i,j] = min(dp[i-1,j] + 1, dp[i,j-1] + 1, dp[i-1,j-1] + cost)\n",
    "    return dp[n,m]\n",
    "\n",
    "def sequence_edit_score(gt, pred):\n",
    "    g = collapse_sequence(gt)\n",
    "    p = collapse_sequence(pred)\n",
    "    if len(g)==0:\n",
    "        return 0.0\n",
    "    dist = levenshtein(g,p)\n",
    "    score = 1.0 - dist / max(len(g), len(p), 1)\n",
    "    return float(score)\n",
    "\n",
    "print('Metrics ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "import torch.optim as optim\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    for batch in tqdm(loader, desc='train'):\n",
    "        feats = batch['feats'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(feats)\n",
    "        B, L, C = logits.shape\n",
    "        loss = criterion(logits.view(B*L, C), labels.view(B*L))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, device, num_classes):\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            feats = batch['feats'].to(device)\n",
    "            labels = batch['labels'].numpy().reshape(-1)\n",
    "            logits = model(feats)\n",
    "            pred = logits.argmax(dim=-1).cpu().numpy().reshape(-1)\n",
    "            preds_all.append(pred)\n",
    "            labels_all.append(labels)\n",
    "    preds = np.concatenate(preds_all)\n",
    "    labels = np.concatenate(labels_all)\n",
    "    mask = labels != -1\n",
    "    labels = labels[mask]; preds = preds[mask]\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    acc = (labels == preds).mean()\n",
    "    return {'f1_macro': float(f1_macro), 'acc': float(acc)}\n",
    "\n",
    "print('Train/val functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training runner (SMOKE-RUN)\n",
    "# Edit cfg above before running. This cell will run training for cfg['training']['epochs'] epochs.\n",
    "set_seed(cfg['training']['seed'])\n",
    "os.makedirs(cfg['io']['out_dir'], exist_ok=True)\n",
    "device = torch.device(cfg['training']['device'] if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = ClipDataset(cfg, split='train')\n",
    "val_ds = ClipDataset(cfg, split='val')\n",
    "print(f'Found {len(train_ds.videos)} videos (sampled), {len(train_ds)} training clips.')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg['training']['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg['training']['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "num_classes = cfg['model']['num_classes']\n",
    "feat_dim = cfg['model']['feat_dim']\n",
    "\n",
    "model = SimpleMS_TCN(feat_dim=feat_dim, hidden_dim=cfg['model']['hidden_dim'],\n",
    "                     num_layers=cfg['model']['num_layers'], num_classes=num_classes).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg['training']['lr'], weight_decay=cfg['training']['weight_decay'])\n",
    "\n",
    "best_val = -1.0\n",
    "for epoch in range(1, cfg['training']['epochs'] + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    metrics = validate(model, val_loader, device, num_classes)\n",
    "    print(f\"Epoch {epoch} train_loss={train_loss:.4f} val_f1={metrics['f1_macro']:.4f} val_acc={metrics['acc']:.4f}\")\n",
    "    ckpt_path = os.path.join(cfg['io']['out_dir'], f\"ckpt_epoch{epoch}.pth\")\n",
    "    torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer': optimizer.state_dict()}, ckpt_path)\n",
    "    if metrics['f1_macro'] > best_val:\n",
    "        best_val = metrics['f1_macro']\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict()}, os.path.join(cfg['io']['out_dir'], 'best.pth'))\n",
    "\n",
    "print('Training complete (smoke-run). Check', cfg['io']['out_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-video inference example (uses best.pth if available)\n",
    "def load_model_from_ckpt(cfg, device, ckpt_path=None):\n",
    "    num_classes = cfg['model']['num_classes']\n",
    "    model = SimpleMS_TCN(feat_dim=cfg['model']['feat_dim'], hidden_dim=cfg['model']['hidden_dim'],\n",
    "                         num_layers=cfg['model']['num_layers'], num_classes=num_classes).to(device)\n",
    "    if ckpt_path is None:\n",
    "        ckpt_path = os.path.join(cfg['io']['out_dir'], 'best.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ck = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ck['model_state'])\n",
    "        print('Loaded checkpoint', ckpt_path)\n",
    "    else:\n",
    "        print('No checkpoint found at', ckpt_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def inference_full_video(model, features, device, seq_len=200, overlap=0.5):\n",
    "    model.eval()\n",
    "    T, D = features.shape\n",
    "    step = int(seq_len * (1 - overlap))\n",
    "    counts = np.zeros((T,), dtype=np.int32)\n",
    "    logits_sum = np.zeros((T, model.classifier.out_channels), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, max(1, T - seq_len + 1), max(1, step)):\n",
    "            clip = torch.from_numpy(features[s:s+seq_len]).unsqueeze(0).to(device).float()\n",
    "            out = model(clip)\n",
    "            out = out.cpu().numpy()[0]\n",
    "            logits_sum[s:s+seq_len] += out\n",
    "            counts[s:s+seq_len] += 1\n",
    "        if counts.sum() == 0:\n",
    "            clip = torch.from_numpy(np.pad(features, ((0, seq_len - T), (0,0)))[None]).to(device).float()\n",
    "            out = model(clip).cpu().numpy()[0][:T]\n",
    "            logits_sum[:T] += out\n",
    "            counts[:T] += 1\n",
    "    logits_avg = logits_sum / counts[:, None]\n",
    "    preds = logits_avg.argmax(axis=-1)\n",
    "    return preds\n",
    "\n",
    "# Example usage:\n",
    "# model = load_model_from_ckpt(cfg, device)\n",
    "# feats = np.load('cataract-101/features/<video_id>.npy')\n",
    "# preds = inference_full_video(model, feats, device, seq_len=cfg['data']['seq_len'])\n",
    "# print('Pred shape', preds.shape)\n",
    "print('Inference cell ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b032569",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "\n",
    "- Edit the config cell at the top to match your dataset paths, feature dim, and number of classes.\n",
    "- If you don't have pre-extracted features, ask me and I will generate a ResNet feature-extraction cell that runs on video files or frames.\n",
    "- For full TeCNO, we can extend this notebook to multi-stage refinement and temporal smoothing post-processing.\n",
    "\n",
    "When ready, run the training cell to perform a smoke training run (3 epochs by default)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
