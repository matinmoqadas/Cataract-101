{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13455357,"sourceType":"datasetVersion","datasetId":8540918}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# If running on Kaggle/Colab, most deps are preinstalled.\n# Uncomment to install anything missing.\n# !pip -q install --upgrade opencv-python-headless==4.10.0.84 timm==1.0.7\n\nimport os\nimport sys\nimport math\nimport time\nimport json\nimport random\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nfrom collections import defaultdict, Counter\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision.models.video import r3d_18, R3D_18_Weights\n\n# Reproducibility\nSEED = 1337\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nprint('Python', sys.version)\nprint('PyTorch', torch.__version__)\nprint('CUDA available:', torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T08:05:16.451457Z","iopub.execute_input":"2025-10-26T08:05:16.451765Z","iopub.status.idle":"2025-10-26T08:05:23.914218Z","shell.execute_reply.started":"2025-10-26T08:05:16.451744Z","shell.execute_reply":"2025-10-26T08:05:23.913288Z"}},"outputs":[{"name":"stdout","text":"Python 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPyTorch 2.6.0+cu124\nCUDA available: True\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === Set your dataset paths ===\n# Example (Kaggle): /kaggle/input/cataract-101/cataract-101\nDATA_ROOT = Path('/kaggle/input/cataract-101/cataract-101')  # <- change if needed\nANN_PATH  = DATA_ROOT / 'annotations.csv'                    # <- change if needed\n\n# Where to save checkpoints & artifacts\nWORK_DIR = Path('/kaggle/working/phase2_clip_baseline')\nWORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# Model/data config\n@dataclass\nclass Config:\n    # Data\n    target_fps: int = 12            # decode at this FPS for training/inference\n    resize: int = 112               # r3d_18 was pretrained at 112x112\n    clip_len_sec: float = 2.0       # ~2 seconds clips\n    stride_frac: float = 0.5        # 50% overlap during training\n    subset_n: Optional[int] = 12    # limit number of videos to fit 20GB; None=all\n    max_frames_per_video: Optional[int] = None  # e.g., 5000 to cap long videos\n\n    # Train\n    epochs: int = 5\n    batch_size: int = 4\n    num_workers: int = 2\n    lr: float = 1e-4\n    weight_decay: float = 1e-4\n    label_smoothing: float = 0.0\n    amp: bool = True\n    grad_accum_steps: int = 1\n\n    # Evaluation/postproc\n    median_smooth: int = 15         # odd window size for timeline smoothing\n\nCFG = Config()\nprint(CFG)\n","metadata":{"trusted":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-10-26T08:05:37.409083Z","iopub.execute_input":"2025-10-26T08:05:37.409368Z","iopub.status.idle":"2025-10-26T08:05:37.417127Z","shell.execute_reply.started":"2025-10-26T08:05:37.409346Z","shell.execute_reply":"2025-10-26T08:05:37.416278Z"},"collapsed":true},"outputs":[{"name":"stdout","text":"Config(target_fps=12, resize=112, clip_len_sec=2.0, stride_frac=0.5, subset_n=12, max_frames_per_video=None, epochs=5, batch_size=4, num_workers=2, lr=0.0001, weight_decay=0.0001, label_smoothing=0.0, amp=True, grad_accum_steps=1, median_smooth=15)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Cell 3 — Robust reader for semicolon Cataract-101 annotations ===\n# Outputs:\n#   df  : FRAME schema → [video_id, frame_idx, phase_id]\n#   videos : {video_id -> Path}\n#   fps_by_vid, num_classes, phase_to_idx, idx_to_phase\n\nimport re, math\nfrom pathlib import Path\nimport pandas as pd, numpy as np\nimport cv2\n\nassert 'DATA_ROOT' in globals(), \"Run Cell 2 first to set DATA_ROOT.\"\nDATA_ROOT = Path(DATA_ROOT)\n\nANN_PATH   = DATA_ROOT / 'annotations.csv'\nVIDEOS_CSV = DATA_ROOT / 'videos.csv'\nPHASES_CSV = DATA_ROOT / 'phases.csv'\nVIDEO_DIR  = DATA_ROOT / 'videos'\nif not ANN_PATH.exists():\n    raise FileNotFoundError(f\"annotations.csv not found at {ANN_PATH}\")\n\n# ---------- helpers ----------\ndef _norm(s: str) -> str:\n    s = str(s).strip().lower().replace('-', '_').replace(' ', '_')\n    return re.sub(r'[^a-z0-9_]', '', s)\n\ndef find_col(df_or_cols, aliases):\n    cols = list(df_or_cols.columns) if hasattr(df_or_cols, \"columns\") else list(df_or_cols)\n    if not cols: return None\n    for a in aliases:\n        if a in cols: return a\n    nmap = {_norm(c): c for c in cols}\n    for a in aliases:\n        na = _norm(a)\n        if na in nmap: return nmap[na]\n    low = {_norm(c): c for c in cols}\n    for a in aliases:\n        na = _norm(a)\n        for lc, orig in low.items():\n            if na in lc: return orig\n    return None\n\n# ---------- videos & fps ----------\nVIDEO_EXTS = {'.mp4', '.mov', '.avi', '.mkv'}\nvideos = {}\nif VIDEO_DIR.exists():\n    for p in VIDEO_DIR.rglob('*'):\n        if p.suffix.lower() in VIDEO_EXTS:\n            videos[p.stem] = p\nprint(f\"[info] found {len(videos)} video files on disk\")\n\ndef get_video_fps(path: Path) -> float:\n    cap = cv2.VideoCapture(str(path))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n    cap.release()\n    return float(fps) if fps and fps > 1e-3 else 25.0\n\nfps_by_vid = {}\nif VIDEOS_CSV.exists():\n    vdf = pd.read_csv(VIDEOS_CSV)\n    vdf.columns = [_norm(c) for c in vdf.columns]\n    vcol = find_col(vdf, ['video_id','video','vid','case_id','filename','file','name'])\n    fcol = find_col(vdf, ['fps','frame_rate','framerate'])\n    if vcol:\n        vdf['__stem__'] = vdf[vcol].astype(str).str.replace(r'\\.[^.]+$', '', regex=True)\n        if fcol:\n            for r in vdf[['__stem__', fcol]].itertuples(index=False):\n                try: fps_by_vid[str(r[0])] = float(r[1])\n                except: pass\n        # ensure stem->file\n        for stem in vdf['__stem__'].unique():\n            if stem not in videos:\n                for p in VIDEO_DIR.rglob('*'):\n                    if p.suffix.lower() in VIDEO_EXTS and p.stem == str(stem):\n                        videos[str(stem)] = p\n\n# ---------- read annotations (force semicolon) ----------\n# Try normal; if a single column or header contains ';', re-read with sep=';'\nann = pd.read_csv(ANN_PATH, engine='python', sep=None)\nneeds_semicolon = (ann.shape[1] == 1) or any(';' in str(c) for c in ann.columns)\nif needs_semicolon:\n    # Try with header; if that still yields 1 column, use header=None then split\n    ann_try = pd.read_csv(ANN_PATH, sep=';', engine='python', on_bad_lines='warn')\n    if ann_try.shape[1] == 1:\n        ann = pd.read_csv(ANN_PATH, sep=';', engine='python', header=None, names=['__one__'])\n        # Split the single column into 3 fields \"video;frame;phase\"\n        split = ann['__one__'].astype(str).str.split(';', n=2, expand=True)\n        if split.shape[1] != 3:\n            raise ValueError(\"Expected 'video;frame;phase' format when splitting semicolon annotations.\")\n        ann = split\n        ann.columns = ['video', 'frame', 'phase']\n    else:\n        ann = ann_try\n\n# normalize column names\nann.columns = [_norm(c) for c in ann.columns]\ncols = set(ann.columns)\nprint(\"[info] annotations columns (normalized):\", sorted(cols))\n\n# If we *still* have 1 column, split it here defensively\nif len(ann.columns) == 1:\n    only = ann.columns[0]\n    if ann[only].astype(str).str.contains(';').any():\n        split = ann[only].astype(str).str.split(';', n=2, expand=True)\n        if split.shape[1] == 3:\n            ann = split\n            ann.columns = ['video','frame','phase']\n            ann.columns = [_norm(c) for c in ann.columns]\n\n# Detect canonical col names now\ncol_video = find_col(ann, ['video','video_id','vid','case_id','filename','file','name'])\ncol_frame = find_col(ann, ['frame','frame_idx','frame_id','frameindex','fid'])\ncol_phase = find_col(ann, ['phase','label','phase_id','class','action','phase_name'])\n\nif not (col_video and col_frame and col_phase):\n    raise ValueError(f\"Could not find video/frame/phase columns in annotations. Got: {list(ann.columns)}\")\n\n# Normalize video ids → stems; then map 269 → case_269\nann[col_video] = ann[col_video].astype(str).str.replace(r'\\.[^.]+$', '', regex=True)\n\ndef _to_case_stem(v):\n    s = str(v).strip()\n    if s.startswith('case_'): return s\n    # if value is something like '269' or 'case269'\n    m = re.search(r'(\\d+)$', s)\n    return f\"case_{m.group(1)}\" if m else s\n\n# Build the FRAME schema df\ntmp = ann[[col_video, col_frame, col_phase]].copy()\ntmp.columns = ['video_id', 'frame_idx', 'phase_raw']\ntmp['video_id'] = tmp['video_id'].map(_to_case_stem)\n\n# phase mapping\nif pd.api.types.is_numeric_dtype(tmp['phase_raw']):\n    tmp['phase_id_raw'] = pd.to_numeric(tmp['phase_raw'], errors='coerce').astype('Int64').fillna(-1).astype(int)\nelse:\n    cats = pd.Categorical(tmp['phase_raw'])\n    tmp['phase_id_raw'] = cats.codes.astype(int)\n    print(\"[info] phase name→id mapping (first 10):\", {int(i): l for i,l in list(enumerate(cats.categories))[:10]})\n\nuniq_raw = pd.Index(sorted(tmp['phase_id_raw'].dropna().unique()))\nphase_to_idx = {int(p): i for i,p in enumerate(uniq_raw)}\nidx_to_phase = {i:int(p) for i,p in enumerate(uniq_raw)}\nnum_classes = len(uniq_raw)\ntmp['phase_id'] = tmp['phase_id_raw'].map(phase_to_idx).astype(int)\n\n# frame_idx as int\ntmp['frame_idx'] = pd.to_numeric(tmp['frame_idx'], errors='coerce').fillna(0).astype(np.int64)\n\n# final df\ndf = tmp[['video_id','frame_idx','phase_id']].sort_values(['video_id','frame_idx']).reset_index(drop=True)\n\n# sanity\nprint(f\"[info] Detected FRAME-LEVEL schema with columns: {df.columns.tolist()}\")\nprint(f\"[info] Num phases = {num_classes}\")\nprint(df.head())\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-10-26T08:05:41.948146Z","iopub.execute_input":"2025-10-26T08:05:41.948456Z","iopub.status.idle":"2025-10-26T08:05:41.975716Z","shell.execute_reply.started":"2025-10-26T08:05:41.948408Z","shell.execute_reply":"2025-10-26T08:05:41.974717Z"},"collapsed":true},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_37/1233736004.py\"\u001b[0;36m, line \u001b[0;32m138\u001b[0m\n\u001b[0;31m    any(k in c for k in keys if isinstance(keys, (list,tuple)) else [keys]):\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (1233736004.py, line 138)","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Map annotation video_ids like \"934\" → \"case_934\"\nassert 'df' in globals(), \"Run Cell 3 first.\"\n\ndef _to_case_stem(v):\n    s = str(v).strip()\n    # already good\n    if s.startswith('case_'):\n        return s\n    # strip any extension just in case\n    s = re.sub(r'\\.[^.]+$', '', s)\n    # keep only trailing digits\n    m = re.search(r'(\\d+)$', s)\n    if m:\n        return f\"case_{m.group(1)}\"\n    return s  # fallback\n\ndf['video_id'] = df['video_id'].map(_to_case_stem)\n\n# Re-check which ids match files we discovered\nmissing = sorted(set(df['video_id']) - set(videos.keys()))\nprint(f\"[alias] after mapping, unmatched annotated videos: {len(missing)}\")\nprint(missing[:20])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unmatched video ids after mapping to case_{id}\nmissing = sorted(set(df['video_id']) - set(videos.keys()))\nprint(f\"[check] unmatched after mapping: {len(missing)}\")\nprint(missing[:25])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Patched Cell — Robust timeline builder (no double phase mapping) ===\nfrom typing import Dict, Optional\nimport numpy as np, math\nfrom pathlib import Path\n\ndef _encode_phase(val, num_classes: Optional[int]=None, phase_to_idx: Optional[Dict[int,int]]=None) -> int:\n    try:\n        v = int(val)\n    except Exception:\n        return 0\n    if num_classes is not None and 0 <= v < num_classes:\n        return v\n    if phase_to_idx and v in phase_to_idx:\n        return int(phase_to_idx[v])\n    if num_classes is not None and num_classes > 0:\n        return int(v) % num_classes\n    return int(max(v, 0))\n\ndef build_timelines(df_ann: pd.DataFrame,\n                    videos: Dict[str, Path],\n                    target_fps: int,\n                    max_frames_per_video: Optional[int]=None,\n                    num_classes: Optional[int]=None,\n                    phase_to_idx: Optional[Dict[int,int]]=None) -> Dict[str, Dict]:\n    \"\"\"\n    Returns: dict[video_id] = {fps: int, n_frames: int, labels: np.ndarray[int]}\n    Works without actual video files; assumes fps=25 if unknown.\n    \"\"\"\n    def default_fps_for(vid: str) -> float:\n        if isinstance(videos, dict) and vid in videos and callable(globals().get(\"get_video_fps\", None)):\n            try:\n                return float(max(1e-3, globals()[\"get_video_fps\"](videos[vid])))\n            except Exception:\n                pass\n        return 25.0\n\n    if num_classes is None and \"phase_id\" in df_ann.columns:\n        try:\n            num_classes = int(df_ann[\"phase_id\"].max()) + 1\n        except Exception:\n            num_classes = None\n\n    timelines: Dict[str, Dict] = {}\n\n    # SEGMENT schema\n    if set([\"video_id\",\"start_sec\",\"end_sec\",\"phase_id\"]).issubset(df_ann.columns):\n        by_vid = df_ann.groupby(\"video_id\", sort=False)\n        for vid, g in by_vid:\n            total_sec = float(max(g[\"end_sec\"].values) if len(g) else 0.0)\n            n_tgt = int(max(1, math.ceil(total_sec * target_fps)))\n            labels = np.full(n_tgt, fill_value=0, dtype=np.int64)\n            for r in g.itertuples(index=False):\n                p = _encode_phase(getattr(r, \"phase_id\"), num_classes, phase_to_idx)\n                a = max(0, int(round(float(getattr(r, \"start_sec\")) * target_fps)))\n                b = min(n_tgt, int(round(float(getattr(r, \"end_sec\"))   * target_fps)))\n                if b <= a:\n                    b = min(n_tgt, a + 1)\n                labels[a:b] = p\n            if max_frames_per_video is not None:\n                labels = labels[:max_frames_per_video]\n            timelines[vid] = dict(fps=target_fps, n_frames=int(labels.shape[0]), labels=labels)\n\n    # FRAME schema\n    elif set([\"video_id\",\"frame_idx\",\"phase_id\"]).issubset(df_ann.columns):\n        by_vid = df_ann.groupby(\"video_id\", sort=False)\n        for vid, g in by_vid:\n            fps_src = default_fps_for(vid)\n            max_src_idx = int(g[\"frame_idx\"].max()) + 1 if len(g) else 1\n            total_sec = max_src_idx / max(fps_src, 1e-6)\n            n_tgt = int(max(1, math.ceil(total_sec * target_fps)))\n            labels = np.full(n_tgt, fill_value=0, dtype=np.int64)\n            for r in g.itertuples(index=False):\n                p  = _encode_phase(getattr(r, \"phase_id\"), num_classes, phase_to_idx)\n                fi = int(getattr(r, \"frame_idx\"))\n                t  = int(round((fi / max(fps_src, 1e-6)) * target_fps))\n                t  = int(np.clip(t, 0, n_tgt - 1))\n                labels[t] = p\n            if max_frames_per_video is not None:\n                labels = labels[:max_frames_per_video]\n            timelines[vid] = dict(fps=target_fps, n_frames=int(labels.shape[0]), labels=labels)\n    else:\n        raise ValueError(\"df must be SEGMENT (video_id,start_sec,end_sec,phase_id) or FRAME (video_id,frame_idx,phase_id) schema.\")\n\n    print('Built timelines for', len(timelines), 'videos')\n    return timelines\n\n# Rebuild with robust function\nTIMELINES = build_timelines(\n    df,\n    videos,\n    target_fps=CFG.target_fps,\n    max_frames_per_video=CFG.max_frames_per_video if hasattr(CFG, \"max_frames_per_video\") else None,\n    num_classes=(num_classes if \"num_classes\" in globals() else None),\n    phase_to_idx=(phase_to_idx if \"phase_to_idx\" in globals() else None),\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_clip_index(timelines: Dict[str, Dict], clip_len_sec: float, stride_frac: float) -> pd.DataFrame:\n    rows = []\n    for vid, info in timelines.items():\n        labels = info['labels']\n        fps = info['fps']\n        L = int(round(clip_len_sec * fps))\n        S = max(1, int(round(L * stride_frac)))\n        if L < 1: \n            continue\n        for a in range(0, len(labels)-L+1, S):\n            b = a + L\n            window = labels[a:b]\n            lab = window[L//2]  # center frame label\n            rows.append((vid, a, b, int(lab)))\n    idx = pd.DataFrame(rows, columns=['video_id','start','end','label'])\n    return idx\n\nCLIP_INDEX = build_clip_index(TIMELINES, CFG.clip_len_sec, CFG.stride_frac)\nprint('Total clips:', len(CLIP_INDEX))\n\n# Train/val split by video (no leakage)\nall_vids = list(TIMELINES.keys())\nrandom.shuffle(all_vids)\nsplit = int(0.8*len(all_vids)) if len(all_vids) > 1 else 1\ntrain_vids, val_vids = set(all_vids[:split]), set(all_vids[split:])\n\ndef split_by_videos(index_df: pd.DataFrame, train_set: set, val_set: set):\n    tr = index_df[index_df.video_id.isin(train_set)].reset_index(drop=True)\n    va = index_df[index_df.video_id.isin(val_set)].reset_index(drop=True)\n    return tr, va\n\nCLIPS_TR, CLIPS_VA = split_by_videos(CLIP_INDEX, train_vids, val_vids)\nprint(CLIPS_TR.shape, CLIPS_VA.shape)\n\n# Class weights from training labels (for imbalance)\ncnt = Counter(CLIPS_TR['label'].tolist())\nclass_counts = np.array([cnt.get(i,0) for i in range(num_classes)])\nclass_weights = 1.0 / np.clip(class_counts, 1, None)\nclass_weights = class_weights / class_weights.sum() * num_classes\nprint('Class counts:', class_counts)\nprint('Class weights:', class_weights.round(3))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nIMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n\nclass ClipDataset(Dataset):\n    def __init__(self, clips_df: pd.DataFrame, timelines: Dict[str, Dict], video_paths: Dict[str, Path],\n                 resize: int, augment: bool, target_fps: int):\n        self.df = clips_df\n        self.timelines = timelines\n        self.videos = video_paths\n        self.size = resize\n        self.augment = augment\n        self.fps = target_fps\n\n    def __len__(self):\n        return len(self.df)\n\n    def _read_window(self, path: Path, a: int, b: int) -> np.ndarray:\n        # Read frames for [a, b) at self.fps using nearest timestamp mapping\n        cap = cv2.VideoCapture(str(path))\n        fps_src = cap.get(cv2.CAP_PROP_FPS) or 25.0\n        frames = []\n        last_frame = None\n        for t in range(a, b):\n            src_idx = int(round((t / self.fps) * fps_src))\n            cap.set(cv2.CAP_PROP_POS_FRAMES, src_idx)\n            ok, frame = cap.read()\n            if not ok:\n                frame = last_frame if last_frame is not None else np.zeros((self.size, self.size, 3), np.uint8)\n            else:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = cv2.resize(frame, (self.size, self.size), interpolation=cv2.INTER_AREA)\n                last_frame = frame\n            frames.append(frame)\n        cap.release()\n        arr = np.stack(frames, axis=0)  # [T,H,W,3]\n        return arr\n\n    def _augment(self, x: np.ndarray) -> np.ndarray:\n        # Simple spatial augs (random horizontal flip)\n        if not self.augment:\n            return x\n        if random.random() < 0.5:\n            x = x[:, :, ::-1, :]\n        return x\n\n    def _normalize(self, x: np.ndarray) -> np.ndarray:\n        x = x.astype(np.float32) / 255.0\n        x = (x - IMAGENET_MEAN) / IMAGENET_STD\n        return x\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        vid, a, b, lab = r.video_id, int(r.start), int(r.end), int(r.label)\n        path = self.videos[vid]\n        x = self._read_window(path, a, b)   # [T,H,W,3]\n        x = self._augment(x)\n        x = self._normalize(x)\n        x = torch.from_numpy(x).permute(3,0,1,2)  # [3,T,H,W]\n        y = torch.tensor(lab, dtype=torch.long)\n        return x, y, vid, a, b\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = ClipDataset(CLIPS_TR, TIMELINES, videos, resize=CFG.resize, augment=True,  target_fps=CFG.target_fps)\nval_ds   = ClipDataset(CLIPS_VA, TIMELINES, videos, resize=CFG.resize, augment=False, target_fps=CFG.target_fps)\n\ntrain_dl = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,  num_workers=CFG.num_workers, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n\nprint('Train clips:', len(train_ds), ' Val clips:', len(val_ds))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pretrained backbone\nweights = R3D_18_Weights.DEFAULT\nnet = r3d_18(weights=weights)\nnet.fc = nn.Linear(net.fc.in_features, num_classes)\n\n# Optional: freeze early layers for speed on small data\n# for name, p in net.named_parameters():\n#     if not name.startswith('layer4') and 'fc' not in name:\n#         p.requires_grad = False\n\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\noptimizer = torch.optim.AdamW(net.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(CFG.epochs,1))\n\nscaler = torch.cuda.amp.GradScaler(enabled=CFG.amp)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.to(device)\nprint('Device:', device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(logits, target):\n    return (logits.argmax(1) == target).float().mean().item()\n\nbest_acc = 0.0\nBEST_PATH = WORK_DIR / 'best_clip_model.pt'\n\nfor epoch in range(1, CFG.epochs+1):\n    net.train()\n    t0 = time.time()\n    tr_loss = 0.0\n    tr_acc = 0.0\n    optimizer.zero_grad(set_to_none=True)\n\n    for step, (x,y,_,_,_) in enumerate(train_dl, 1):\n        x = x.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=CFG.amp):\n            logits = net(x)\n            loss = F.cross_entropy(logits, y, label_smoothing=CFG.label_smoothing, reduction='mean')\n        scaler.scale(loss/CFG.grad_accum_steps).backward()\n        if step % CFG.grad_accum_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n        tr_loss += loss.item()\n        tr_acc  += accuracy(logits.detach(), y)\n\n    scheduler.step()\n\n    # Validation\n    net.eval()\n    va_loss = 0.0\n    va_acc = 0.0\n    with torch.no_grad():\n        for x,y,_,_,_ in val_dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=CFG.amp):\n                logits = net(x)\n                loss = F.cross_entropy(logits, y, label_smoothing=0.0, reduction='mean')\n            va_loss += loss.item()\n            va_acc  += accuracy(logits, y)\n\n    ntr = max(1, len(train_dl)); nva = max(1, len(val_dl))\n    tr_loss/=ntr; tr_acc/=ntr; va_loss/=nva; va_acc/=nva\n    dt = time.time()-t0\n\n    print(f\"Epoch {epoch:02d} | {dt:5.1f}s | \"\n          f\"train loss {tr_loss:.3f} acc {tr_acc:.3f} | val loss {va_loss:.3f} acc {va_acc:.3f}\")\n\n    if va_acc > best_acc:\n        best_acc = va_acc\n        torch.save({'model': net.state_dict(),\n                    'num_classes': num_classes,\n                    'phase_to_idx': phase_to_idx}, BEST_PATH)\n        print('  -> saved best to', BEST_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net.eval()\nall_rows = []\nwith torch.no_grad():\n    for x,y,vid,a,b in val_dl:\n        x = x.to(device)\n        logits = net(x)\n        probs = torch.softmax(logits, dim=1).cpu().numpy()\n        pred = probs.argmax(axis=1)\n        for i in range(len(vid)):\n            all_rows.append((vid[i], int(a[i]), int(b[i]), int(pred[i])))\n\nVAL_PREDS = pd.DataFrame(all_rows, columns=['video_id','start','end','pred'])\nprint(VAL_PREDS.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import medfilt\n\ndef stitch_predictions(pred_df: pd.DataFrame, timelines: Dict[str, Dict], num_classes: int,\n                       smooth: int = 15) -> Dict[str, np.ndarray]:\n    pred_timelines = {}\n    for vid, g in pred_df.groupby('video_id'):\n        if vid not in timelines:\n            continue\n        n = timelines[vid]['n_frames']\n        votes = np.zeros((num_classes, n), dtype=np.int32)\n        for r in g.itertuples(index=False):\n            votes[r.pred, r.start:r.end] += 1\n        out = votes.argmax(axis=0)\n        if smooth and smooth % 2 == 1 and smooth > 1:\n            out = medfilt(out, kernel_size=smooth)\n        pred_timelines[vid] = out\n    return pred_timelines\n\nPRED_TIMELINES = stitch_predictions(VAL_PREDS, TIMELINES, num_classes, smooth=CFG.median_smooth)\nprint('Pred timelines for', len(PRED_TIMELINES), 'videos')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef eval_frame_metrics(pred_tl: Dict[str, np.ndarray], gt: Dict[str, Dict]) -> Dict[str, float]:\n    y_true = []\n    y_pred = []\n    for vid, pred in pred_tl.items():\n        if vid not in gt:\n            continue\n        ref = gt[vid]['labels']\n        m = min(len(ref), len(pred))\n        y_true.append(ref[:m])\n        y_pred.append(pred[:m])\n    if not y_true:\n        return dict(frame_acc=float('nan'), macro_f1=float('nan'))\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    acc = (y_true == y_pred).mean().item()\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return dict(frame_acc=acc, macro_f1=macro_f1)\n\nmetrics = eval_frame_metrics(PRED_TIMELINES, TIMELINES)\nprint(metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot ground truth vs. predicted for one validation video\nval_example_vid = next(iter(set(CLIPS_VA.video_id))) if len(CLIPS_VA) else None\nif val_example_vid is not None and val_example_vid in PRED_TIMELINES:\n    y_gt = TIMELINES[val_example_vid]['labels']\n    y_pr = PRED_TIMELINES[val_example_vid]\n    T = min(len(y_gt), len(y_pr))\n    t = np.arange(T) / CFG.target_fps\n\n    plt.figure(figsize=(14,3))\n    plt.plot(t, y_gt[:T], lw=2, label='GT', alpha=0.9)\n    plt.plot(t, y_pr[:T], lw=1, label='Pred', alpha=0.9)\n    plt.xlabel('Seconds')\n    plt.ylabel('Phase idx')\n    plt.title(f'Video: {val_example_vid} — GT vs Predicted (smoothed={CFG.median_smooth})')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('No validation example available to visualize.')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model (optional; already loaded during training)\nckpt = torch.load(WORK_DIR / 'best_clip_model.pt', map_location='cpu')\nnet.load_state_dict(ckpt['model'])\nnet.eval()\nprint('Loaded best model with classes:', ckpt['num_classes'])\n\n# Export per-frame predictions for all VAL videos (CSV)\nrows = []\nfor vid, arr in PRED_TIMELINES.items():\n    for i, p in enumerate(arr):\n        rows.append((vid, i, p))\n\ndf_out = pd.DataFrame(rows, columns=['video_id','t_idx','pred_label_idx'])\nCSV_OUT = WORK_DIR / 'val_frame_predictions.csv'\ndf_out.to_csv(CSV_OUT, index=False)\nprint('Saved:', CSV_OUT)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enforce a minimum duration per segment to reduce rapid label jitter.\ndef apply_min_duration(arr: np.ndarray, min_len_frames: int) -> np.ndarray:\n    x = arr.copy()\n    i = 0\n    n = len(x)\n    while i < n:\n        j = i\n        while j < n and x[j] == x[i]:\n            j += 1\n        run_len = j - i\n        if run_len < min_len_frames:\n            left = x[i-1] if i-1 >= 0 else x[j if j < n else i]\n            right = x[j] if j < n else x[i-1]\n            fill = left if left == right else (left if i>0 else right)\n            x[i:j] = fill\n        i = j\n    return x\n\n# Example usage on one video\nval_example_vid = next(iter(PRED_TIMELINES.keys())) if PRED_TIMELINES else None\nif val_example_vid is not None:\n    pred0 = PRED_TIMELINES[val_example_vid]\n    pred1 = apply_min_duration(pred0, min_len_frames=int(1.0*CFG.target_fps))  # 1s minimum\n    print('Changed frames:', int((pred0!=pred1).sum()))\nelse:\n    print('No predictions to post-process.')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}