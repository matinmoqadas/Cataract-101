{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13455357,
          "sourceType": "datasetVersion",
          "datasetId": 8540918
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "cataract101_clip_classifier",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "matinmo_cataract_101_path = kagglehub.dataset_download('matinmo/cataract-101')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "8rAXjca3Nq1o"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Rv-bT1GENq1u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cataract-101 Surgical Phase Recognition Prototype\n",
        "\n",
        "This notebook builds a lightweight clip-based classifier for surgical phase recognition using the Cataract-101 dataset. It leverages a pretrained ResNet50 backbone applied on per-frame images, aggregates temporal features, and supports rapid experimentation on Kaggle by sampling a subset of videos.\n"
      ],
      "metadata": {
        "id": "ie0ZN5RvNq1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 2\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from einops import rearrange\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display\n",
        "\n",
        "# Configuration\n",
        "DATA_ROOT = Path(\"/kaggle/input/cataract-101/cataract-101\")\n",
        "VIDEOS_DIR = DATA_ROOT / \"videos\"\n",
        "PHASE_FILE = DATA_ROOT / \"phases.csv\"\n",
        "VIDEO_META_FILE = DATA_ROOT / \"videos.csv\"\n",
        "ANNOTATION_FILE = DATA_ROOT / \"annotations.csv\"\n",
        "OUTPUT_DIR = Path(\"/kaggle/working/output\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED = 1337\n",
        "SAMPLE_VIDEOS = 8\n",
        "SAMPLE_FRACTION = None  # e.g., 0.3 to use 30% of the videos\n",
        "TRAIN_VIDEO_LIMIT = SAMPLE_VIDEOS\n",
        "VAL_VIDEO_LIMIT = max(2, SAMPLE_VIDEOS // 4) if SAMPLE_VIDEOS else None\n",
        "\n",
        "CLIP_LEN = 16\n",
        "FRAME_STEP = 2            # temporal stride between frames inside a clip\n",
        "CLIP_STRIDE = max(1, CLIP_LEN // 2)  # stride between consecutive clips\n",
        "MAX_CLIPS_PER_VIDEO = None            # optionally cap number of clips per video\n",
        "FRAME_SAMPLING_STEP = 1              # expand annotations into dense frame indices\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "VAL_BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "GRAD_ACCUM_STEPS = 1\n",
        "USE_AMP = torch.cuda.is_available()\n",
        "FREEZE_BACKBONE = False\n",
        "TRAINABLE_BACKBONE_LAYERS = (\"layer4\",)\n",
        "DROPOUT = 0.2\n",
        "\n",
        "INFERENCE_BATCH_SIZE = 4\n",
        "INFERENCE_CLIP_STRIDE = CLIP_STRIDE\n",
        "SAVE_BEST_MODEL_PATH = OUTPUT_DIR / \"best_clip_resnet50.pt\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pd.options.display.max_columns = 50\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        "print(f\"Videos directory: {VIDEOS_DIR}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "4ij6L0lVNq11"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 3\n",
        "phase_df = pd.read_csv(PHASE_FILE, sep=';')\n",
        "phase_df[\"Phase\"] = phase_df[\"Phase\"].astype(int)\n",
        "phase_df[\"Meaning\"] = phase_df[\"Meaning\"].astype(str)\n",
        "phase_df[\"PhaseName\"] = phase_df.apply(lambda row: f\"{int(row['Phase'])}: {row['Meaning']}\", axis=1)\n",
        "phase_to_name = dict(zip(phase_df[\"Phase\"], phase_df[\"PhaseName\"]))\n",
        "phase_to_idx = {int(phase_id): idx for idx, phase_id in enumerate(sorted(phase_to_name.keys()))}\n",
        "IDX_TO_PHASE = {idx: phase_to_name[phase_id] for phase_id, idx in phase_to_idx.items()}\n",
        "PHASE_NAMES = [IDX_TO_PHASE[idx] for idx in range(len(IDX_TO_PHASE))]\n",
        "print(f\"Detected {len(PHASE_NAMES)} phases: {PHASE_NAMES}\")\n",
        "display(phase_df)\n",
        "\n",
        "video_meta_df = pd.read_csv(VIDEO_META_FILE, sep=';')\n",
        "video_meta_df[\"VideoID\"] = video_meta_df[\"VideoID\"].astype(int)\n",
        "video_meta_df[\"Frames\"] = video_meta_df[\"Frames\"].astype(int)\n",
        "video_meta_df[\"FPS\"] = pd.to_numeric(video_meta_df[\"FPS\"], errors='coerce').fillna(25.0)\n",
        "video_meta_df[\"Surgeon\"] = video_meta_df[\"Surgeon\"].astype(int)\n",
        "video_meta_df[\"Experience\"] = video_meta_df[\"Experience\"].astype(int)\n",
        "print(f\"Video metadata entries: {len(video_meta_df)}\")\n",
        "display(video_meta_df.head())\n",
        "\n",
        "annotation_df = pd.read_csv(ANNOTATION_FILE, sep=';')\n",
        "annotation_df[\"VideoID\"] = annotation_df[\"VideoID\"].astype(int)\n",
        "annotation_df[\"FrameNo\"] = annotation_df[\"FrameNo\"].astype(int)\n",
        "annotation_df[\"Phase\"] = annotation_df[\"Phase\"].astype(int)\n",
        "annotation_df[\"PhaseIdx\"] = annotation_df[\"Phase\"].map(phase_to_idx)\n",
        "if annotation_df[\"PhaseIdx\"].isna().any():\n",
        "    missing_rows = annotation_df[annotation_df[\"PhaseIdx\"].isna()].head()\n",
        "    print(\"[WARN] Some annotations reference unknown phases; previewing the first few rows:\")\n",
        "    display(missing_rows)\n",
        "    annotation_df = annotation_df.dropna(subset=[\"PhaseIdx\"]).copy()\n",
        "annotation_df[\"PhaseIdx\"] = annotation_df[\"PhaseIdx\"].astype(int)\n",
        "print(f\"Annotation rows: {len(annotation_df):,}\")\n",
        "display(annotation_df.head())\n",
        "\n",
        "phase_counts = annotation_df[\"PhaseIdx\"].value_counts().sort_index()\n",
        "phase_count_df = pd.DataFrame({\"phase\": [IDX_TO_PHASE[idx] for idx in phase_counts.index],\n",
        "                               \"count\": phase_counts.values})\n",
        "display(phase_count_df)\n",
        "\n",
        "video_files = sorted(VIDEOS_DIR.glob(\"case_*.mp4\"))\n",
        "print(f\"Discovered {len(video_files)} video files under {VIDEOS_DIR}.\")\n",
        "if video_files:\n",
        "    preview_df = pd.DataFrame({\"video_path\": [str(p.relative_to(DATA_ROOT)) for p in video_files[:5]]})\n",
        "    display(preview_df)\n",
        "\n",
        "missing_meta = sorted(set(annotation_df[\"VideoID\"]) - set(video_meta_df[\"VideoID\"]))\n",
        "missing_files = [vid for vid in annotation_df[\"VideoID\"].unique()\n",
        "                 if not (VIDEOS_DIR / f\"case_{vid}.mp4\").exists()]\n",
        "print(f\"Videos in annotations without metadata: {len(missing_meta)}\")\n",
        "print(f\"Videos with missing files: {len(missing_files)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "jPZ9lnXHNq15"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 4\n",
        "@dataclass\n",
        "class VideoRecord:\n",
        "    video_id: str\n",
        "    video_numeric_id: int\n",
        "    media_path: Path\n",
        "    total_frames: int\n",
        "    fps: float\n",
        "    frame_numbers: np.ndarray\n",
        "    phase_indices: np.ndarray\n",
        "    segments: List[Tuple[int, int, int]]  # (start_frame, end_frame, phase_idx)\n",
        "    annotation: pd.DataFrame\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ClipSample:\n",
        "    video_id: str\n",
        "    frame_indices: np.ndarray\n",
        "    label: int\n",
        "\n",
        "\n",
        "def build_video_records(\n",
        "    annotations: pd.DataFrame,\n",
        "    video_meta: pd.DataFrame,\n",
        "    videos_dir: Path,\n",
        "    frame_sampling_step: int = 1,\n",
        ") -> List[VideoRecord]:\n",
        "    records: List[VideoRecord] = []\n",
        "    meta_index: Dict[int, pd.Series] = {int(row.VideoID): row for _, row in video_meta.iterrows()}\n",
        "    for video_id, group in annotations.groupby(\"VideoID\"):\n",
        "        video_numeric = int(video_id)\n",
        "        meta = meta_index.get(video_numeric)\n",
        "        if meta is None:\n",
        "            print(f\"[WARN] VideoID {video_numeric} has annotations but no metadata; skipping.\")\n",
        "            continue\n",
        "        media_path = videos_dir / f\"case_{video_numeric}.mp4\"\n",
        "        if not media_path.exists():\n",
        "            print(f\"[WARN] Media file {media_path} not found; skipping video {video_numeric}.\")\n",
        "            continue\n",
        "        total_frames = int(meta[\"Frames\"])\n",
        "        fps = float(meta.get(\"FPS\", 25.0)) if not pd.isna(meta.get(\"FPS\", np.nan)) else 25.0\n",
        "        group_sorted = group.sort_values(\"FrameNo\").reset_index(drop=True)\n",
        "        start_frames = group_sorted[\"FrameNo\"].to_numpy(dtype=np.int32)\n",
        "        phase_indices = group_sorted[\"PhaseIdx\"].to_numpy(dtype=np.int32)\n",
        "        if start_frames.size == 0:\n",
        "            print(f\"[WARN] Video {video_numeric} has no frame transitions; skipping.\")\n",
        "            continue\n",
        "        if start_frames[0] > 0:\n",
        "            start_frames = np.insert(start_frames, 0, 0)\n",
        "            phase_indices = np.insert(phase_indices, 0, phase_indices[0])\n",
        "        segment_ends = np.append(start_frames[1:] - 1, total_frames - 1)\n",
        "        segments: List[Tuple[int, int, int]] = []\n",
        "        sampled_frames: List[np.ndarray] = []\n",
        "        sampled_labels: List[np.ndarray] = []\n",
        "        for start, end, phase_idx in zip(start_frames, segment_ends, phase_indices):\n",
        "            start = int(max(0, start))\n",
        "            end = int(min(end, total_frames - 1))\n",
        "            if end < start:\n",
        "                end = start\n",
        "            segments.append((start, end, int(phase_idx)))\n",
        "            indices = np.arange(start, end + 1, frame_sampling_step, dtype=np.int32)\n",
        "            if indices.size == 0:\n",
        "                indices = np.array([start], dtype=np.int32)\n",
        "            sampled_frames.append(indices)\n",
        "            sampled_labels.append(np.full(indices.shape, int(phase_idx), dtype=np.int32))\n",
        "        frame_numbers = np.concatenate(sampled_frames)\n",
        "        label_array = np.concatenate(sampled_labels)\n",
        "        order = np.argsort(frame_numbers)\n",
        "        frame_numbers = frame_numbers[order]\n",
        "        label_array = label_array[order]\n",
        "        records.append(\n",
        "            VideoRecord(\n",
        "                video_id=str(video_numeric),\n",
        "                video_numeric_id=video_numeric,\n",
        "                media_path=media_path,\n",
        "                total_frames=total_frames,\n",
        "                fps=fps,\n",
        "                frame_numbers=frame_numbers,\n",
        "                phase_indices=label_array,\n",
        "                segments=segments,\n",
        "                annotation=group_sorted.copy(),\n",
        "            )\n",
        "        )\n",
        "    records.sort(key=lambda rec: rec.video_numeric_id)\n",
        "    print(f\"Prepared metadata for {len(records)} videos.\")\n",
        "    return records\n",
        "\n",
        "\n",
        "def subset_records(\n",
        "    records: Sequence[VideoRecord],\n",
        "    limit: Optional[int] = None,\n",
        "    fraction: Optional[float] = None,\n",
        "    seed: int = SEED,\n",
        ") -> List[VideoRecord]:\n",
        "    if not records:\n",
        "        return []\n",
        "    rng = random.Random(seed)\n",
        "    indices = list(range(len(records)))\n",
        "    rng.shuffle(indices)\n",
        "    if fraction is not None:\n",
        "        limit = max(1, int(len(records) * fraction))\n",
        "    if limit is not None:\n",
        "        indices = indices[: min(limit, len(indices))]\n",
        "    return [records[i] for i in sorted(indices)]\n",
        "\n",
        "\n",
        "def split_train_val(\n",
        "    records: Sequence[VideoRecord],\n",
        "    val_ratio: float = 0.2,\n",
        "    seed: int = SEED,\n",
        ") -> Tuple[List[VideoRecord], List[VideoRecord]]:\n",
        "    records = list(records)\n",
        "    if not records:\n",
        "        return [], []\n",
        "    rng = random.Random(seed)\n",
        "    rng.shuffle(records)\n",
        "    val_count = max(1, int(len(records) * val_ratio)) if len(records) > 1 else 1\n",
        "    val_records = records[:val_count]\n",
        "    train_records = records[val_count:]\n",
        "    if not train_records:\n",
        "        train_records, val_records = val_records, train_records\n",
        "    print(f\"Split into {len(train_records)} train and {len(val_records)} validation videos.\")\n",
        "    return train_records, val_records\n",
        "\n",
        "\n",
        "class ClipDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        records: Sequence[VideoRecord],\n",
        "        transform: transforms.Compose,\n",
        "        clip_len: int = 16,\n",
        "        frame_step: int = 1,\n",
        "        clip_stride: int = 8,\n",
        "        max_clips_per_video: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        self.records = list(records)\n",
        "        self.transform = transform\n",
        "        self.clip_len = clip_len\n",
        "        self.frame_step = max(1, frame_step)\n",
        "        self.clip_stride = max(1, clip_stride)\n",
        "        self.max_clips_per_video = max_clips_per_video\n",
        "        self.samples: List[ClipSample] = []\n",
        "        self.video_map: Dict[str, VideoRecord] = {rec.video_id: rec for rec in self.records}\n",
        "        self._build_index()\n",
        "\n",
        "    def _build_index(self) -> None:\n",
        "        for rec in self.records:\n",
        "            frames = rec.frame_numbers\n",
        "            labels = rec.phase_indices\n",
        "            if len(frames) < self.clip_len:\n",
        "                continue\n",
        "            max_start = len(frames) - (self.clip_len - 1) * self.frame_step\n",
        "            if max_start <= 0:\n",
        "                continue\n",
        "            start_positions = list(range(0, max_start, self.clip_stride))\n",
        "            final_candidate = max_start - 1\n",
        "            if start_positions:\n",
        "                if final_candidate > start_positions[-1]:\n",
        "                    start_positions.append(final_candidate)\n",
        "            else:\n",
        "                start_positions = [0]\n",
        "            clip_count = 0\n",
        "            for start in start_positions:\n",
        "                stop = start + self.clip_len * self.frame_step\n",
        "                indices = frames[start:stop:self.frame_step]\n",
        "                if len(indices) < self.clip_len:\n",
        "                    continue\n",
        "                label_slice = labels[start:stop:self.frame_step]\n",
        "                if len(label_slice) < self.clip_len:\n",
        "                    continue\n",
        "                majority_label = Counter(label_slice).most_common(1)[0][0]\n",
        "                self.samples.append(\n",
        "                    ClipSample(\n",
        "                        video_id=rec.video_id,\n",
        "                        frame_indices=indices.astype(np.int64),\n",
        "                        label=int(majority_label),\n",
        "                    )\n",
        "                )\n",
        "                clip_count += 1\n",
        "                if self.max_clips_per_video and clip_count >= self.max_clips_per_video:\n",
        "                    break\n",
        "        print(f\"Built {len(self.samples)} clip samples from {len(self.records)} videos.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _load_clip_tensor(self, rec: VideoRecord, frame_indices: np.ndarray) -> torch.Tensor:\n",
        "        cap = cv2.VideoCapture(str(rec.media_path))\n",
        "        if not cap.isOpened():\n",
        "            raise RuntimeError(f\"Unable to open video at {rec.media_path}\")\n",
        "        frames: List[torch.Tensor] = []\n",
        "        for idx in frame_indices:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            image = Image.fromarray(frame_rgb)\n",
        "            frames.append(self.transform(image))\n",
        "        cap.release()\n",
        "        if len(frames) != len(frame_indices):\n",
        "            if frames:\n",
        "                while len(frames) < len(frame_indices):\n",
        "                    frames.append(frames[-1])\n",
        "            else:\n",
        "                raise RuntimeError(f\"Failed to read any frames for video {rec.video_id}\")\n",
        "        clip_tensor = torch.stack(frames, dim=0)  # (T, C, H, W)\n",
        "        return clip_tensor\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        sample = self.samples[idx]\n",
        "        record = self.video_map[sample.video_id]\n",
        "        clip_tensor = self._load_clip_tensor(record, sample.frame_indices)\n",
        "        return clip_tensor, sample.label\n",
        "\n",
        "\n",
        "def build_ground_truth_timeline(record: VideoRecord) -> np.ndarray:\n",
        "    timeline = np.full(record.total_frames, fill_value=-1, dtype=np.int32)\n",
        "    for start, end, label_idx in record.segments:\n",
        "        start = int(max(0, start))\n",
        "        end = int(min(end, record.total_frames - 1))\n",
        "        timeline[start : end + 1] = label_idx\n",
        "    if (timeline == -1).any():\n",
        "        valid_indices = np.where(timeline != -1)[0]\n",
        "        if valid_indices.size:\n",
        "            first_label = timeline[valid_indices[0]]\n",
        "            last_label = timeline[valid_indices[-1]]\n",
        "            timeline[: valid_indices[0]] = first_label\n",
        "            timeline[valid_indices[-1] + 1 :] = last_label\n",
        "    return timeline\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "HyTgLCEoNq19"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 5\n",
        "video_records = build_video_records(\n",
        "    annotations=annotation_df,\n",
        "    video_meta=video_meta_df,\n",
        "    videos_dir=VIDEOS_DIR,\n",
        "    frame_sampling_step=FRAME_SAMPLING_STEP,\n",
        ")\n",
        "\n",
        "train_records, val_records = split_train_val(video_records, val_ratio=0.2, seed=SEED)\n",
        "train_records = subset_records(train_records, limit=TRAIN_VIDEO_LIMIT, fraction=SAMPLE_FRACTION, seed=SEED)\n",
        "val_records = subset_records(val_records, limit=VAL_VIDEO_LIMIT, fraction=None, seed=SEED)\n",
        "\n",
        "print(f\"Detected phases ({len(PHASE_NAMES)}): {PHASE_NAMES}\")\n",
        "print(f\"Using {len(train_records)} videos for training and {len(val_records)} for validation.\")\n",
        "print(f\"Train videos (sample): {[rec.video_id for rec in train_records[:5]]}\")\n",
        "print(f\"Validation videos (sample): {[rec.video_id for rec in val_records[:5]]}\")\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "train_dataset = ClipDataset(\n",
        "    records=train_records,\n",
        "    transform=train_transform,\n",
        "    clip_len=CLIP_LEN,\n",
        "    frame_step=FRAME_STEP,\n",
        "    clip_stride=CLIP_STRIDE,\n",
        "    max_clips_per_video=MAX_CLIPS_PER_VIDEO,\n",
        ")\n",
        "\n",
        "val_dataset = ClipDataset(\n",
        "    records=val_records,\n",
        "    transform=val_transform,\n",
        "    clip_len=CLIP_LEN,\n",
        "    frame_step=FRAME_STEP,\n",
        "    clip_stride=CLIP_STRIDE,\n",
        "    max_clips_per_video=MAX_CLIPS_PER_VIDEO,\n",
        ")\n",
        "\n",
        "print(f\"Train clips: {len(train_dataset)}, Validation clips: {len(val_dataset)}\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "if len(train_dataset) > 0:\n",
        "    sample_clip, sample_label = train_dataset[0]\n",
        "    print(f\"Sample clip shape: {sample_clip.shape}, label index: {sample_label} ({PHASE_NAMES[sample_label]})\")\n",
        "else:\n",
        "    print(\"Warning: training dataset is empty; adjust sampling parameters or verify annotations.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "Q6bvcmPjNq2C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 6\n",
        "class ClipResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        pretrained: bool = True,\n",
        "        dropout: float = 0.2,\n",
        "        freeze_backbone: bool = False,\n",
        "        trainable_layers: Sequence[str] = (\"layer4\",),\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        backbone = models.resnet50(weights=weights)\n",
        "        if freeze_backbone:\n",
        "            for param in backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "            for layer_name in trainable_layers:\n",
        "                layer = getattr(backbone, layer_name, None)\n",
        "                if layer is not None:\n",
        "                    for param in layer.parameters():\n",
        "                        param.requires_grad = True\n",
        "        self.feature_dim = backbone.fc.in_features\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        self.dropout = nn.Dropout(dropout) if dropout else nn.Identity()\n",
        "        self.head = nn.Linear(self.feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() != 5:\n",
        "            raise ValueError(f\"Expected input shape (B, T, C, H, W) but received {tuple(x.shape)}\")\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = rearrange(x, \"b t c h w -> (b t) c h w\")\n",
        "        feats = self.backbone(x)\n",
        "        feats = torch.flatten(feats, 1)\n",
        "        feats = feats.view(b, t, self.feature_dim)\n",
        "        clip_features = feats.mean(dim=1)\n",
        "        clip_features = self.dropout(clip_features)\n",
        "        logits = self.head(clip_features)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = ClipResNet(\n",
        "    num_classes=len(PHASE_NAMES),\n",
        "    pretrained=True,\n",
        "    dropout=DROPOUT,\n",
        "    freeze_backbone=FREEZE_BACKBONE,\n",
        "    trainable_layers=TRAINABLE_BACKBONE_LAYERS,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, NUM_EPOCHS))\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "print(model)\n",
        "print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "Qc-VkFzINq2F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 7\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n",
        "    grad_accum_steps: int = 1,\n",
        ") -> Dict[str, float]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [train]\", leave=False)\n",
        "    for step, (clips, targets) in enumerate(pbar):\n",
        "        clips = clips.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None and USE_AMP):\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss = loss / grad_accum_steps\n",
        "        if scaler is not None and USE_AMP:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        if (step + 1) % grad_accum_steps == 0:\n",
        "            if scaler is not None and USE_AMP:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        running_loss += loss.item() * grad_accum_steps\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        running_correct += (preds == targets).sum().item()\n",
        "        running_total += targets.size(0)\n",
        "        pbar.set_postfix(loss=running_loss / (step + 1),\n",
        "                         acc=running_correct / max(1, running_total))\n",
        "    return {\n",
        "        \"loss\": running_loss / max(1, len(dataloader)),\n",
        "        \"acc\": running_correct / max(1, running_total),\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    all_preds: List[torch.Tensor] = []\n",
        "    all_targets: List[torch.Tensor] = []\n",
        "    for clips, targets in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
        "        clips = clips.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, targets)\n",
        "        running_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        running_correct += (preds == targets).sum().item()\n",
        "        running_total += targets.size(0)\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_targets.append(targets.cpu())\n",
        "    if all_preds:\n",
        "        preds_tensor = torch.cat(all_preds).numpy()\n",
        "        targets_tensor = torch.cat(all_targets).numpy()\n",
        "    else:\n",
        "        preds_tensor = np.array([])\n",
        "        targets_tensor = np.array([])\n",
        "    return {\n",
        "        \"loss\": running_loss / max(1, len(dataloader)),\n",
        "        \"acc\": running_correct / max(1, running_total),\n",
        "        \"preds\": preds_tensor,\n",
        "        \"targets\": targets_tensor,\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "TXlNLyyRNq2I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 8\n",
        "if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "    raise RuntimeError(\"Training or validation dataset is empty. Adjust sampling parameters or verify the metadata before proceeding.\")\n",
        "\n",
        "history: List[Dict[str, float]] = []\n",
        "best_state = copy.deepcopy(model.state_dict())\n",
        "best_metrics = None\n",
        "best_acc = -math.inf\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_metrics = train_one_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        epoch=epoch,\n",
        "        scaler=scaler,\n",
        "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
        "    )\n",
        "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "    history.append({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": train_metrics[\"loss\"],\n",
        "        \"train_acc\": train_metrics[\"acc\"],\n",
        "        \"val_loss\": val_metrics[\"loss\"],\n",
        "        \"val_acc\": val_metrics[\"acc\"],\n",
        "    })\n",
        "    print(\n",
        "        f\"Epoch {epoch}: \"\n",
        "        f\"train_loss={train_metrics['loss']:.4f}, train_acc={train_metrics['acc']:.3f}, \"\n",
        "        f\"val_loss={val_metrics['loss']:.4f}, val_acc={val_metrics['acc']:.3f}\"\n",
        "    )\n",
        "    current_acc = val_metrics[\"acc\"]\n",
        "    if current_acc >= best_acc:\n",
        "        best_acc = current_acc\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "        best_metrics = val_metrics\n",
        "        torch.save(best_state, SAVE_BEST_MODEL_PATH)\n",
        "        print(f\"  -> New best model saved to {SAVE_BEST_MODEL_PATH}\")\n",
        "\n",
        "history_df = pd.DataFrame(history)\n",
        "display(history_df)\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"Loaded best model with validation accuracy {best_acc:.3f}\")\n",
        "else:\n",
        "    print(\"Warning: best model state was not captured; using final epoch weights.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "LmTPGzFkNq2O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 9\n",
        "def load_clip_from_video(\n",
        "    video_path: Path,\n",
        "    frame_indices: Sequence[int],\n",
        "    transform: transforms.Compose,\n",
        ") -> torch.Tensor:\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Unable to open {video_path}\")\n",
        "    frames: List[torch.Tensor] = []\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(transform(Image.fromarray(frame_rgb)))\n",
        "    cap.release()\n",
        "    if len(frames) != len(frame_indices):\n",
        "        if frames:\n",
        "            while len(frames) < len(frame_indices):\n",
        "                frames.append(frames[-1])\n",
        "        else:\n",
        "            raise RuntimeError(f\"Failed to read requested frames from {video_path}\")\n",
        "    return torch.stack(frames, dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_video_timeline(\n",
        "    model: nn.Module,\n",
        "    record: VideoRecord,\n",
        "    transform: transforms.Compose,\n",
        "    clip_len: int,\n",
        "    frame_step: int,\n",
        "    clip_stride: int,\n",
        "    device: torch.device,\n",
        "    batch_size: int = 4,\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    model.eval()\n",
        "    video_path = record.media_path\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = record.total_frames\n",
        "    fps = record.fps if record.fps else 25.0\n",
        "    if cap.isOpened():\n",
        "        total_frames_cap = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps_cap = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if total_frames_cap > 0:\n",
        "            total_frames = total_frames_cap\n",
        "        if fps_cap and fps_cap > 0:\n",
        "            fps = fps_cap\n",
        "    cap.release()\n",
        "    if total_frames <= 0:\n",
        "        raise RuntimeError(f\"Unable to determine total frame count for {video_path}\")\n",
        "\n",
        "    clip_span = (clip_len - 1) * frame_step + 1\n",
        "    if total_frames < clip_span:\n",
        "        clip_start_positions = [0]\n",
        "    else:\n",
        "        clip_start_positions = list(range(0, total_frames - clip_span + 1, clip_stride))\n",
        "        final_candidate = total_frames - clip_span\n",
        "        if clip_start_positions[-1] != final_candidate:\n",
        "            clip_start_positions.append(final_candidate)\n",
        "    clip_indices = [list(range(start, min(total_frames, start + clip_span), frame_step))\n",
        "                    for start in clip_start_positions]\n",
        "\n",
        "    logits_batches: List[torch.Tensor] = []\n",
        "    buffer: List[torch.Tensor] = []\n",
        "    clip_tracker: List[int] = []\n",
        "\n",
        "    for start, indices in zip(clip_start_positions, clip_indices):\n",
        "        clip_tensor = load_clip_from_video(video_path, indices, transform)\n",
        "        buffer.append(clip_tensor)\n",
        "        clip_tracker.append(start)\n",
        "        if len(buffer) == batch_size:\n",
        "            batch = torch.stack(buffer, dim=0).to(device, non_blocking=True)\n",
        "            outputs = model(batch)\n",
        "            logits_batches.append(outputs.cpu())\n",
        "            buffer.clear()\n",
        "    if buffer:\n",
        "        batch = torch.stack(buffer, dim=0).to(device, non_blocking=True)\n",
        "        outputs = model(batch)\n",
        "        logits_batches.append(outputs.cpu())\n",
        "        buffer.clear()\n",
        "\n",
        "    if not logits_batches:\n",
        "        raise RuntimeError(\"No clips were generated for inference.\")\n",
        "    logits = torch.cat(logits_batches, dim=0)\n",
        "    probs = torch.softmax(logits, dim=1).numpy()\n",
        "    preds = probs.argmax(axis=1)\n",
        "\n",
        "    frame_votes = np.zeros((total_frames, probs.shape[1]), dtype=np.float32)\n",
        "    for start, indices, prob_vec in zip(clip_tracker, clip_indices, probs):\n",
        "        for idx in indices:\n",
        "            if idx < total_frames:\n",
        "                frame_votes[idx] += prob_vec\n",
        "    frame_labels = frame_votes.argmax(axis=1)\n",
        "    frame_confidence = frame_votes.max(axis=1)\n",
        "\n",
        "    return {\n",
        "        \"frame_labels\": frame_labels,\n",
        "        \"frame_confidence\": frame_confidence,\n",
        "        \"frame_votes\": frame_votes,\n",
        "        \"clip_probs\": probs,\n",
        "        \"clip_preds\": preds,\n",
        "        \"clip_starts\": np.array(clip_tracker),\n",
        "        \"fps\": fps,\n",
        "        \"total_frames\": total_frames,\n",
        "    }\n",
        "\n",
        "\n",
        "timeline_outputs = None\n",
        "sample_record = val_records[0] if val_records else (train_records[0] if train_records else None)\n",
        "if sample_record is None:\n",
        "    print(\"No video records available for inference demo.\")\n",
        "else:\n",
        "    timeline_outputs = predict_video_timeline(\n",
        "        model=model,\n",
        "        record=sample_record,\n",
        "        transform=val_transform,\n",
        "        clip_len=CLIP_LEN,\n",
        "        frame_step=FRAME_STEP,\n",
        "        clip_stride=INFERENCE_CLIP_STRIDE,\n",
        "        device=device,\n",
        "        batch_size=INFERENCE_BATCH_SIZE,\n",
        "    )\n",
        "    print(f\"Inference complete for video '{sample_record.video_id}' with {timeline_outputs['total_frames']} frames.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "PRFBTtrgNq2R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 10\n",
        "if best_metrics and best_metrics[\"preds\"].size > 0:\n",
        "    cm = confusion_matrix(\n",
        "        best_metrics[\"targets\"],\n",
        "        best_metrics[\"preds\"],\n",
        "        labels=list(range(len(PHASE_NAMES))),\n",
        "    )\n",
        "    cm_df = pd.DataFrame(cm, index=PHASE_NAMES, columns=PHASE_NAMES)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(\"Validation Confusion Matrix\")\n",
        "    plt.ylabel(\"True Phase\")\n",
        "    plt.xlabel(\"Predicted Phase\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    report = classification_report(\n",
        "        best_metrics[\"targets\"],\n",
        "        best_metrics[\"preds\"],\n",
        "        target_names=PHASE_NAMES,\n",
        "        digits=3,\n",
        "        zero_division=0,\n",
        "    )\n",
        "    print(report)\n",
        "else:\n",
        "    print(\"Confusion matrix is unavailable; validation predictions were empty.\")\n",
        "\n",
        "if sample_record is not None and timeline_outputs is not None:\n",
        "    total_frames = timeline_outputs[\"total_frames\"]\n",
        "    pred_labels = timeline_outputs[\"frame_labels\"]\n",
        "    pred_conf = timeline_outputs[\"frame_confidence\"]\n",
        "    gt_timeline = build_ground_truth_timeline(sample_record)\n",
        "    frame_axis = np.arange(total_frames)\n",
        "    time_axis = frame_axis / max(1e-6, timeline_outputs[\"fps\"])\n",
        "\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    plt.step(time_axis, gt_timeline, where=\"post\", label=\"Ground truth\", linewidth=2, alpha=0.8)\n",
        "    plt.step(time_axis, pred_labels, where=\"post\", label=\"Predicted\", linewidth=1.5, alpha=0.8)\n",
        "    plt.yticks(range(len(PHASE_NAMES)), PHASE_NAMES)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Phase\")\n",
        "    plt.title(f\"Surgical phase timeline â€” {sample_record.video_id}\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(16, 2))\n",
        "    plt.plot(time_axis, pred_conf, label=\"Prediction confidence\")\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "    plt.ylabel(\"Confidence\")\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.title(\"Frame-level confidence\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Timeline visualization is unavailable; ensure inference ran successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "RN_3enZMNq2U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TJ7qPG5TNq2Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "crA90JWpNq2Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "BGVkL9x9Nq2Z"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}