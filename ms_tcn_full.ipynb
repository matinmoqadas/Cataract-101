{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dde26a3",
   "metadata": {},
   "source": [
    "# Full MS-TCN Prototype Notebook\n",
    "\n",
    "Contains: preprocessing (build segments), ResNet feature extraction (from videos), dataset, model (MS-TCN), training, and evaluation cells.\n",
    "\n",
    "**Paths**: change `BASE_DIR` and `WORK_DIR` in the config cell to match your environment (Kaggle defaults provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cacbc1",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install packages if missing:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision numpy pandas scikit-learn pyyaml opencv-python tqdm nbformat\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "BASE_DIR = \"/kaggle/input/cataract-101/cataract-101\"   # where annotations.csv and videos/ live (Kaggle)\n",
    "WORK_DIR = \"/kaggle/working/cataract-101-generated\"   # where we will write segments, features, outputs\n",
    "FPS = 25\n",
    "FEATURE_BACKBONE = \"resnet50\"   # resnet18 or resnet50\n",
    "PRETRAINED_BACKBONE = True\n",
    "BATCH_FRAME = 64   # frames per batch for feature extraction\n",
    "IMG_SIZE = 224\n",
    "SEQ_LEN = 200\n",
    "SAMPLE_VIDEOS = 30   # None to use all videos\n",
    "NUM_CLASSES = None   # set later after reading phases / annotations\n",
    "DEVICE = \"cuda\" if __import__('torch').cuda.is_available() else \"cpu\"\n",
    "PRINT_DEBUG = True\n",
    "\n",
    "import os\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(WORK_DIR, \"features\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(WORK_DIR, \"labels_npy\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(WORK_DIR, \"checkpoints\"), exist_ok=True)\n",
    "print('Config:', BASE_DIR, '->', WORK_DIR, 'device=', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, sys, math, time, shutil, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "print('imports ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed92d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: Build segments_filled.csv and per-video label arrays ===\n",
    "from pathlib import Path\n",
    "ann_path = Path(BASE_DIR) / \"annotations.csv\"\n",
    "phases_path = Path(BASE_DIR) / \"phases.csv\"\n",
    "videos_dir = Path(BASE_DIR) / \"videos\"\n",
    "if not ann_path.exists():\n",
    "    raise FileNotFoundError(f\"annotations.csv not found at {ann_path}\")\n",
    "\n",
    "ann = pd.read_csv(ann_path)\n",
    "print('annotations columns:', list(ann.columns))\n",
    "\n",
    "# load phases\n",
    "phase_name_to_id = {}\n",
    "if phases_path.exists():\n",
    "    phases_df = pd.read_csv(phases_path)\n",
    "    if phases_df.shape[1] == 2:\n",
    "        id_col = phases_df.columns[0]; name_col = phases_df.columns[1]\n",
    "        for _, r in phases_df.iterrows():\n",
    "            phase_name_to_id[str(r[name_col])] = int(r[id_col])\n",
    "    else:\n",
    "        for i, r in phases_df.iterrows():\n",
    "            phase_name_to_id[str(r.iloc[-1])] = i\n",
    "    print('Loaded phases mapping from phases.csv:', phase_name_to_id)\n",
    "else:\n",
    "    print('No phases.csv found; will infer phase ids from annotations.')\n",
    "\n",
    "def find_col(df, keys):\n",
    "    for k in keys:\n",
    "        for c in df.columns:\n",
    "            if k in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "col_video = find_col(ann, ['video','video_id','videoid','vid','file','filename'])\n",
    "col_phase = find_col(ann, ['phase','label','phase_id','phaseid','class','action','phase_name'])\n",
    "col_start = find_col(ann, ['start_frame','startframe','start_f','start'])\n",
    "col_end = find_col(ann, ['end_frame','endframe','end_f','end'])\n",
    "col_frame = find_col(ann, ['frame','frame_id','frameid'])\n",
    "col_start_time = find_col(ann, ['start_time','startsec','start_s','startseconds'])\n",
    "col_end_time = find_col(ann, ['end_time','endsec','end_s','endseconds'])\n",
    "\n",
    "print('Detected columns:', dict(video=col_video, phase=col_phase, start=col_start, end=col_end, frame=col_frame,\n",
    "                               start_time=col_start_time, end_time=col_end_time))\n",
    "\n",
    "segments = []\n",
    "if col_start is not None and col_end is not None:\n",
    "    for _, r in ann.iterrows():\n",
    "        vid = str(r[col_video])\n",
    "        phase_val = r[col_phase] if col_phase is not None else 0\n",
    "        start_v = r[col_start]; end_v = r[col_end]\n",
    "        def to_frame(x):\n",
    "            if pd.isnull(x): return None\n",
    "            try:\n",
    "                xf = float(x)\n",
    "            except:\n",
    "                s = str(x)\n",
    "                if ':' in s:\n",
    "                    parts = [float(p) for p in s.split(':')]\n",
    "                    if len(parts) == 3:\n",
    "                        secs = parts[0]*3600 + parts[1]*60 + parts[2]\n",
    "                    elif len(parts) == 2:\n",
    "                        secs = parts[0]*60 + parts[1]\n",
    "                    else:\n",
    "                        secs = parts[0]\n",
    "                    return int(round(secs * FPS))\n",
    "                return None\n",
    "            if abs(xf - round(xf)) < 1e-6 and xf >= 0 and xf > 100:\n",
    "                return int(round(xf))\n",
    "            if xf <= 20:\n",
    "                return int(round(xf * FPS))\n",
    "            return int(round(xf))\n",
    "        sfrm = to_frame(start_v); efrm = to_frame(end_v)\n",
    "        if sfrm is None or efrm is None: continue\n",
    "        if phase_name_to_id:\n",
    "            key = str(phase_val)\n",
    "            if key in phase_name_to_id:\n",
    "                pid = phase_name_to_id[key]\n",
    "            else:\n",
    "                try:\n",
    "                    pid = int(phase_val)\n",
    "                except:\n",
    "                    pid = len(phase_name_to_id)\n",
    "                    phase_name_to_id[key] = pid\n",
    "        else:\n",
    "            try:\n",
    "                pid = int(phase_val)\n",
    "            except:\n",
    "                key = str(phase_val)\n",
    "                if key not in phase_name_to_id:\n",
    "                    phase_name_to_id[key] = len(phase_name_to_id)\n",
    "                pid = phase_name_to_id[key]\n",
    "        segments.append({'video_id': vid, 'start_frame': int(sfrm), 'end_frame': int(efrm), 'phase_id': int(pid)})\n",
    "elif col_frame is not None and col_phase is not None:\n",
    "    grouped = ann.groupby(col_video)\n",
    "    for vid, g in grouped:\n",
    "        g_sorted = g.sort_values(by=col_frame)\n",
    "        frames = g_sorted[col_frame].astype(int).values\n",
    "        phases = g_sorted[col_phase].astype(str).values\n",
    "        if len(frames)==0: continue\n",
    "        curr_phase = phases[0]; curr_start = frames[0]; prev = frames[0]\n",
    "        for f, ph in zip(frames[1:], phases[1:]):\n",
    "            if ph != curr_phase or f != prev + 1:\n",
    "                if curr_phase not in phase_name_to_id:\n",
    "                    phase_name_to_id[curr_phase] = len(phase_name_to_id)\n",
    "                pid = phase_name_to_id[curr_phase]\n",
    "                segments.append({'video_id': str(vid), 'start_frame': int(curr_start), 'end_frame': int(prev), 'phase_id': int(pid)})\n",
    "                curr_phase = ph; curr_start = f\n",
    "            prev = f\n",
    "        if curr_phase not in phase_name_to_id:\n",
    "            phase_name_to_id[curr_phase] = len(phase_name_to_id)\n",
    "        pid = phase_name_to_id[curr_phase]\n",
    "        segments.append({'video_id': str(vid), 'start_frame': int(curr_start), 'end_frame': int(prev), 'phase_id': int(pid)})\n",
    "else:\n",
    "    raise ValueError('Could not detect usable annotation format in annotations.csv. Inspect columns and adapt script.')\n",
    "\n",
    "seg_df = pd.DataFrame(segments)\n",
    "seg_df = seg_df[seg_df['end_frame'] >= seg_df['start_frame']].reset_index(drop=True)\n",
    "seg_out = Path(WORK_DIR) / 'segments_filled.csv'\n",
    "seg_df.to_csv(seg_out, index=False)\n",
    "print('Saved', seg_out, 'segments:', len(seg_df), 'videos:', seg_df.video_id.nunique())\n",
    "\n",
    "# build per-video label arrays\n",
    "labels_dir = Path(WORK_DIR) / 'labels_npy'\n",
    "labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "for vid, g in seg_df.groupby('video_id'):\n",
    "    max_frame = int(g['end_frame'].max()) + 1\n",
    "    labels = np.full((max_frame,), -1, dtype=np.int32)\n",
    "    for _, r in g.iterrows():\n",
    "        s = int(r['start_frame']); e = int(r['end_frame'])\n",
    "        labels[s:e+1] = int(r['phase_id'])\n",
    "    np.save(labels_dir / f'{vid}.npy', labels)\n",
    "print('Saved labels_npy for', len(list(labels_dir.glob('*.npy'))), 'videos at', labels_dir)\n",
    "\n",
    "if globals().get('NUM_CLASSES', None) is None:\n",
    "    used_ids = sorted(seg_df['phase_id'].unique().tolist())\n",
    "    NUM = int(max(used_ids) + 1) if len(used_ids)>0 else 1\n",
    "    globals()['NUM_CLASSES'] = NUM\n",
    "print('NUM_CLASSES set to', globals()['NUM_CLASSES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: ResNet feature extraction from videos -> features/{video_id}.npy ===\n",
    "from pathlib import Path\n",
    "videos_csv = Path(BASE_DIR) / 'videos.csv'\n",
    "videoid_to_file = {}\n",
    "if videos_csv.exists():\n",
    "    vids_df = pd.read_csv(videos_csv)\n",
    "    cols = [c.lower() for c in vids_df.columns]\n",
    "    vidcol = None; filecol = None\n",
    "    for i,c in enumerate(cols):\n",
    "        if 'video' in c or 'id' in c or 'name' in c:\n",
    "            vidcol = vids_df.columns[i]; break\n",
    "    for i,c in enumerate(cols):\n",
    "        if 'file' in c or 'path' in c or 'video' in c:\n",
    "            filecol = vids_df.columns[i]; break\n",
    "    if vidcol is None:\n",
    "        vidcol = vids_df.columns[0]\n",
    "    if filecol is None:\n",
    "        filecol = vids_df.columns[-1]\n",
    "    for _, r in vids_df.iterrows():\n",
    "        videoid_to_file[str(r[vidcol])] = str(r[filecol])\n",
    "    if PRINT_DEBUG:\n",
    "        print('Loaded videos.csv mapping for', len(videoid_to_file), 'entries')\n",
    "else:\n",
    "    print('No videos.csv found; will try to infer filenames from videos folder')\n",
    "\n",
    "video_files = {}\n",
    "for p in Path(BASE_DIR).joinpath('videos').iterdir():\n",
    "    if p.is_file():\n",
    "        stem = p.stem\n",
    "        video_files[stem] = str(p)\n",
    "\n",
    "all_video_ids = sorted([p.stem for p in Path(WORK_DIR).joinpath('labels_npy').glob('*.npy')])\n",
    "print('label videos:', all_video_ids[:5], '... total', len(all_video_ids))\n",
    "\n",
    "for vid in all_video_ids:\n",
    "    if vid in videoid_to_file:\n",
    "        path = Path(BASE_DIR) / 'videos' / videoid_to_file[vid]\n",
    "        if path.exists():\n",
    "            video_files[vid] = str(path)\n",
    "    if vid not in video_files:\n",
    "        for ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
    "            cand = Path(BASE_DIR) / 'videos' / (vid + ext)\n",
    "            if cand.exists():\n",
    "                video_files[vid] = str(cand)\n",
    "                break\n",
    "\n",
    "print('Found video files for', len(video_files), 'videos (of', len(all_video_ids), 'labelled videos)')\n",
    "\n",
    "# build backbone\n",
    "print('Building backbone:', FEATURE_BACKBONE, 'pretrained=', PRETRAINED_BACKBONE)\n",
    "if FEATURE_BACKBONE.lower().startswith('resnet'):\n",
    "    if FEATURE_BACKBONE=='resnet50':\n",
    "        backbone = models.resnet50(pretrained=PRETRAINED_BACKBONE)\n",
    "    else:\n",
    "        backbone = models.resnet18(pretrained=PRETRAINED_BACKBONE)\n",
    "    modules = list(backbone.children())[:-1]\n",
    "    backbone = nn.Sequential(*modules)\n",
    "    backbone.eval()\n",
    "    backbone.to(DEVICE)\n",
    "    feat_dim = 2048 if FEATURE_BACKBONE=='resnet50' else 512\n",
    "else:\n",
    "    raise ValueError('Only resnet backbones supported in this notebook')\n",
    "print('Backbone ready, feat_dim=', feat_dim)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def extract_features_from_video(video_path, out_path, label_len=None, batch_frames=BATCH_FRAME):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print('Failed to open', video_path); return False\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if len(frames)==0:\n",
    "        print('No frames in', video_path); return False\n",
    "    if label_len is not None and len(frames) > label_len:\n",
    "        frames = frames[:label_len]\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(frames), batch_frames):\n",
    "            batch = frames[i:i+batch_frames]\n",
    "            tensor_batch = torch.stack([transform(f) for f in batch]).to(DEVICE)\n",
    "            out = backbone(tensor_batch)\n",
    "            out = out.view(out.size(0), -1).cpu().numpy()\n",
    "            feats.append(out)\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    np.save(out_path, feats)\n",
    "    return True\n",
    "\n",
    "features_dir = Path(WORK_DIR) / 'features'\n",
    "features_dir.mkdir(parents=True, exist_ok=True)\n",
    "count_done = 0\n",
    "for vid in tqdm(all_video_ids[:SAMPLE_VIDEOS] if SAMPLE_VIDEOS is not None else all_video_ids):\n",
    "    outp = features_dir / f'{vid}.npy'\n",
    "    if outp.exists():\n",
    "        count_done += 1\n",
    "        continue\n",
    "    if vid not in video_files:\n",
    "        print('No source video file found for', vid, '; skipping feature extraction')\n",
    "        continue\n",
    "    lab = np.load(Path(WORK_DIR) / 'labels_npy' / f'{vid}.npy')\n",
    "    label_len = lab.shape[0]\n",
    "    ok = extract_features_from_video(video_files[vid], outp, label_len=label_len, batch_frames=BATCH_FRAME)\n",
    "    if ok:\n",
    "        count_done += 1\n",
    "print('Feature extraction done for', count_done, 'videos. Saved to', features_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: Dataset, model, training & eval ===\n",
    "# Dataset\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, work_dir, seq_len=SEQ_LEN, sample_videos=SAMPLE_VIDEOS, split='train', seed=42):\n",
    "        self.work_dir = Path(work_dir)\n",
    "        self.features_dir = self.work_dir / 'features'\n",
    "        self.labels_dir = self.work_dir / 'labels_npy'\n",
    "        self.segments_csv = self.work_dir / 'segments_filled.csv'\n",
    "        self.seq_len = seq_len\n",
    "        self.sample_videos = sample_videos\n",
    "        self.seed = seed\n",
    "\n",
    "        seg_df = pd.read_csv(self.segments_csv)\n",
    "        self.video_labels = {}\n",
    "        for vid, g in seg_df.groupby('video_id'):\n",
    "            max_frame = int(g['end_frame'].max()) + 1\n",
    "            labels = np.full((max_frame,), -1, dtype=np.int32)\n",
    "            for _, r in g.iterrows():\n",
    "                s = int(r['start_frame']); e = int(r['end_frame'])\n",
    "                labels[s:e+1] = int(r['phase_id'])\n",
    "            self.video_labels[str(vid)] = labels\n",
    "\n",
    "        self.videos = sorted(list(self.video_labels.keys()))\n",
    "        random.seed(self.seed); random.shuffle(self.videos)\n",
    "        if self.sample_videos is not None:\n",
    "            self.videos = self.videos[:self.sample_videos]\n",
    "\n",
    "        self.index = []\n",
    "        for vid in self.videos:\n",
    "            n = len(self.video_labels[vid])\n",
    "            if n < self.seq_len: continue\n",
    "            for s in range(0, n - self.seq_len + 1, self.seq_len):\n",
    "                self.index.append((vid, s))\n",
    "        if len(self.index)==0:\n",
    "            for vid in self.videos:\n",
    "                n = len(self.video_labels[vid])\n",
    "                if n >= self.seq_len:\n",
    "                    for s in range(0, n - self.seq_len + 1, max(1, self.seq_len//2)):\n",
    "                        self.index.append((vid, s))\n",
    "\n",
    "    def __len__(self): return len(self.index)\n",
    "    def __getitem__(self, idx):\n",
    "        vid, s = self.index[idx]\n",
    "        feat_path = self.features_dir / f'{vid}.npy'\n",
    "        if not feat_path.exists():\n",
    "            raise FileNotFoundError(f'Missing features for {vid} at {feat_path}')\n",
    "        feats = np.load(feat_path)\n",
    "        clip_feats = feats[s:s+self.seq_len]\n",
    "        clip_lbls = self.video_labels[vid][s:s+self.seq_len]\n",
    "        return {'video_id': vid, 'start': s, 'feats': torch.from_numpy(clip_feats).float(), 'labels': torch.from_numpy(clip_lbls).long()}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = torch.stack([b['feats'] for b in batch], dim=0)\n",
    "    labels = torch.stack([b['labels'] for b in batch], dim=0)\n",
    "    return {'feats': feats, 'labels': labels, 'video_id': [b['video_id'] for b in batch], 'start': [b['start'] for b in batch]}\n",
    "\n",
    "# Model\n",
    "class DilatedResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__()\n",
    "        self.conv_dilated = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=dilation, dilation=dilation)\n",
    "        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "    def forward(self, x):\n",
    "        out = self.conv_dilated(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv_1x1(out)\n",
    "        out = self.dropout(out)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return self.relu(out + x)\n",
    "\n",
    "class SimpleMS_TCN(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_conv = nn.Conv1d(feat_dim, hidden_dim, kernel_size=1)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** (i % 4)\n",
    "            layers.append(DilatedResidualLayer(hidden_dim, hidden_dim, dilation=dilation))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Conv1d(hidden_dim, num_classes, kernel_size=1)\n",
    "    def forward(self, feats):\n",
    "        x = feats.permute(0,2,1)\n",
    "        x = self.input_conv(x)\n",
    "        x = self.tcn(x)\n",
    "        out = self.classifier(x)\n",
    "        out = out.permute(0,2,1)\n",
    "        return out\n",
    "\n",
    "# Metrics helpers\n",
    "def compute_frame_metrics(gt, pred, num_classes):\n",
    "    mask = gt != -1\n",
    "    gt = gt[mask]; pred = pred[mask]\n",
    "    acc = float((gt==pred).mean())\n",
    "    p, r, f1, _ = precision_recall_fscore_support(gt, pred, labels=list(range(num_classes)), zero_division=0)\n",
    "    macro_f1 = float(np.nanmean(f1))\n",
    "    return {'acc': acc, 'macro_f1': macro_f1}\n",
    "\n",
    "def collapse_sequence(x):\n",
    "    out=[]\n",
    "    for v in x:\n",
    "        if v==-1: continue\n",
    "        if len(out)==0 or out[-1]!=v: out.append(int(v))\n",
    "    return out\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    n,m = len(a), len(b)\n",
    "    dp = np.zeros((n+1,m+1), dtype=int)\n",
    "    for i in range(1,n+1): dp[i,0]=i\n",
    "    for j in range(1,m+1): dp[0,j]=j\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i,j] = min(dp[i-1,j]+1, dp[i,j-1]+1, dp[i-1,j-1]+cost)\n",
    "    return dp[n,m]\n",
    "\n",
    "def sequence_edit_score(gt,pred):\n",
    "    g = collapse_sequence(gt); p = collapse_sequence(pred)\n",
    "    if len(g)==0: return 0.0\n",
    "    dist = levenshtein(g,p)\n",
    "    score = 1.0 - dist / max(len(g), len(p), 1)\n",
    "    return float(score)\n",
    "\n",
    "print('Dataset+Model+Metrics ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d295cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: Train (SMOKE RUN) ===\n",
    "work_dir = WORK_DIR\n",
    "seq_len = SEQ_LEN\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "hidden_dim = 64\n",
    "num_layers = 6\n",
    "num_classes = globals().get('NUM_CLASSES', 10)\n",
    "\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "train_ds = ClipDataset(work_dir, seq_len=seq_len, sample_videos=SAMPLE_VIDEOS, split='train', seed=42)\n",
    "val_ds = ClipDataset(work_dir, seq_len=seq_len, sample_videos=max(1, min(10, SAMPLE_VIDEOS//3 or 1)), split='val', seed=999)\n",
    "print('Train videos:', len(train_ds.videos), 'Train clips:', len(train_ds))\n",
    "print('Val videos:', len(val_ds.videos), 'Val clips:', len(val_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "first_feat = list(Path(work_dir).joinpath('features').glob('*.npy'))[0]\n",
    "feat_dim = np.load(first_feat).shape[1]\n",
    "print('Detected feat_dim=', feat_dim)\n",
    "\n",
    "model = SimpleMS_TCN(feat_dim=feat_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "best_val = -1.0\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'train-epoch{epoch}'):\n",
    "        feats = batch['feats'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        logits = model(feats)\n",
    "        B,L,C = logits.shape\n",
    "        loss = criterion(logits.view(B*L,C), labels.view(B*L))\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "    avg_loss = total_loss / len(train_loader) if len(train_loader)>0 else 0.0\n",
    "    model.eval()\n",
    "    preds_all=[]; labels_all=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            feats = batch['feats'].to(device)\n",
    "            labels = batch['labels'].numpy().reshape(-1)\n",
    "            logits = model(feats)\n",
    "            pred = logits.argmax(dim=-1).cpu().numpy().reshape(-1)\n",
    "            preds_all.append(pred); labels_all.append(labels)\n",
    "    if len(preds_all)>0:\n",
    "        preds = np.concatenate(preds_all); labels = np.concatenate(labels_all)\n",
    "        mask = labels != -1\n",
    "        val_f1 = 0.0; val_acc = 0.0\n",
    "        if mask.sum()>0:\n",
    "            from sklearn.metrics import f1_score\n",
    "            val_f1 = f1_score(labels[mask], preds[mask], average='macro', zero_division=0)\n",
    "            val_acc = float((labels[mask]==preds[mask]).mean())\n",
    "    else:\n",
    "        val_f1 = 0.0; val_acc = 0.0\n",
    "    print(f'Epoch {epoch}: train_loss={avg_loss:.4f} val_f1={val_f1:.4f} val_acc={val_acc:.4f}')\n",
    "    ckpt = {'epoch': epoch, 'model_state': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    ckpt_path = Path(work_dir) / 'checkpoints' / f'ckpt_epoch{epoch}.pth'\n",
    "    torch.save(ckpt, ckpt_path)\n",
    "    if val_f1 > best_val:\n",
    "        best_val = val_f1\n",
    "        torch.save(ckpt, Path(work_dir) / 'checkpoints' / 'best.pth')\n",
    "\n",
    "print('Training finished. Best val f1=', best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10baa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: Full-video inference + metrics ===\n",
    "def load_model(checkpoint_path, device):\n",
    "    ck = torch.load(checkpoint_path, map_location=device)\n",
    "    m = SimpleMS_TCN(feat_dim=feat_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "    m.load_state_dict(ck['model_state'])\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "def inference_full_video(model, features, device, seq_len=SEQ_LEN, overlap=0.5):\n",
    "    T,D = features.shape\n",
    "    step = int(seq_len * (1 - overlap))\n",
    "    counts = np.zeros((T,), dtype=np.int32)\n",
    "    logits_sum = np.zeros((T, num_classes), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, max(1, T - seq_len + 1), max(1, step)):\n",
    "            clip = torch.from_numpy(features[s:s+seq_len]).unsqueeze(0).to(device).float()\n",
    "            out = model(clip).cpu().numpy()[0]\n",
    "            L = out.shape[0]\n",
    "            logits_sum[s:s+L] += out\n",
    "            counts[s:s+L] += 1\n",
    "        if counts.sum() == 0:\n",
    "            clip = torch.from_numpy(np.pad(features, ((0, seq_len-T), (0,0)))[None]).to(device).float()\n",
    "            out = model(clip).cpu().numpy()[0][:T]\n",
    "            logits_sum[:T] += out\n",
    "            counts[:T] += 1\n",
    "    logits_avg = logits_sum / counts[:, None]\n",
    "    preds = logits_avg.argmax(axis=-1)\n",
    "    return preds\n",
    "\n",
    "best_ckpt = Path(WORK_DIR) / 'checkpoints' / 'best.pth'\n",
    "if not best_ckpt.exists():\n",
    "    ckpts = sorted(Path(WORK_DIR).joinpath('checkpoints').glob('ckpt_epoch*.pth'))\n",
    "    if len(ckpts)>0:\n",
    "        best_ckpt = ckpts[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError('No checkpoint found in', Path(WORK_DIR).joinpath('checkpoints'))\n",
    "\n",
    "# load model\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "model = load_model(str(best_ckpt), device)\n",
    "features_dir = Path(WORK_DIR) / 'features'\n",
    "labels_dir = Path(WORK_DIR) / 'labels_npy'\n",
    "out_preds_dir = Path(WORK_DIR) / 'preds_npy'\n",
    "out_preds_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "for feat_file in sorted(features_dir.glob('*.npy')):\n",
    "    vid = feat_file.stem\n",
    "    feats = np.load(feat_file)\n",
    "    preds = inference_full_video(model, feats, device, seq_len=SEQ_LEN)\n",
    "    np.save(out_preds_dir / f'{vid}_pred.npy', preds)\n",
    "    gt_path = labels_dir / f'{vid}.npy'\n",
    "    if gt_path.exists():\n",
    "        gt = np.load(gt_path)\n",
    "        L = min(len(gt), len(preds))\n",
    "        fm = compute_frame_metrics(gt[:L], preds[:L], num_classes)\n",
    "        edit = sequence_edit_score(gt[:L], preds[:L])\n",
    "        rows.append({'video_id': vid, 'acc': fm['acc'], 'macro_f1': fm['macro_f1'], 'edit': edit})\n",
    "    else:\n",
    "        rows.append({'video_id': vid, 'acc': None, 'macro_f1': None, 'edit': None})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(Path(WORK_DIR) / 'inference_metrics.csv', index=False)\n",
    "print('Saved inference metrics to', Path(WORK_DIR) / 'inference_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd9e89",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Edit the configuration cell and run cells sequentially. If you want a smaller notebook without feature extraction (assumes you already have features), tell me and I'll generate that variant."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
