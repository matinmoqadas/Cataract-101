{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13455357,"sourceType":"datasetVersion","datasetId":8540918}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0 — hugging face token\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"hf_token\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1 — Environment, imports, Hugging Face API init, logging, system probes\n# -----------------------------------------------------------------------------\n# Purpose: central imports and HF init. Uses psutil + pynvml (if available) to monitor memory.\nimport os\nimport sys\nimport time\nimport math\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Dict, List, Any\n\n# Core data / vision / torch imports\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\n\n# optional monitoring libs (psutil and pynvml recommended for accurate metrics)\ntry:\n    import psutil\nexcept Exception:\n    psutil = None\n\ntry:\n    import pynvml\n    pynvml.nvmlInit()\n    _NVML_AVAILABLE = True\nexcept Exception:\n    _NVML_AVAILABLE = False\n\n# Hugging Face API\nfrom huggingface_hub import HfApi\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\nlog = logging.getLogger(\"cataract_pipeline\")\n\n# Initialize HF API client using HF_TOKEN env var; user can set HF_TOKEN in notebook runtime\n# HF_TOKEN = os.getenv(\"HF_TOKEN\") or os.getenv(\"HF_HUB_TOKEN\")\nif not HF_TOKEN:\n    log.warning(\"HF_TOKEN not found in environment — uploading will fail until HF_TOKEN is provided.\")\nhf_api = HfApi(token=HF_TOKEN)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 — Configuration (tweak as needed)\n# -----------------------------------------------------------------------------\n# Comments: set paths, memory thresholds, multi-GPU options, and other tuning.\nCONFIG = dict(\n    BASE_DIR=\"/kaggle/input/cataract-101/cataract-101\",   # change if dataset is elsewhere\n    WORK_DIR=\"/kaggle/working/cataract-101-generated\",\n    FPS=10,\n    FEATURE_BACKBONE=\"resnet50\",\n    PRETRAINED_BACKBONE=True,\n    BATCH_FRAME=8,            # initial frames per model batch for encoding (will adapt on OOM)\n    IMG_SIZE=112,\n    SEQ_LEN=200,\n    SAMPLE_VIDEOS=4,       # limit processed videos for testing\n    PRINT_DEBUG=True,\n    MEMMAP_DIR=\"/kaggle/working/cataract-101-generated/memmaps\",\n    # Memory safety thresholds: fraction of RAM/VRAM usage at which processing will stop (0..1)\n    MAX_RAM_FRAC=0.90,        # stop when host RAM >= this fraction\n    MAX_VRAM_FRAC=0.92,       # stop when any GPU's VRAM >= this fraction\n    MIN_BATCH_FRAME=1,        # don't reduce batch below this\n    GPU_DEVICE_IDS=None,      # None => autodetect\n    SAFE_SLEEP_SEC=0.1,       # tiny sleep in loops to yield\n    HF_REPO_ID=\"Mateo4/cataract-101-generated\",\n    HF_REPO_TYPE=\"dataset\",\n)\n# Create WORK_DIR early so later cells can assume it exists\nfrom pathlib import Path\nWORK_DIR = Path(CONFIG[\"WORK_DIR\"])\nWORK_DIR.mkdir(parents=True, exist_ok=True)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 — Hardware / memory utilities and printer (read often)\n# -----------------------------------------------------------------------------\n# Comments: helper functions to inspect RAM and GPU VRAM, plus a guard function\ndef host_ram_usage_frac() -> float:\n    \"\"\"Return fraction of used host RAM (0..1).\"\"\"\n    if psutil is None:\n        # fallback: unknown, be conservative\n        return 0.0\n    mem = psutil.virtual_memory()\n    return float(mem.used) / float(mem.total)\n\ndef gpu_memory_info() -> List[Dict[str, Any]]:\n    \"\"\"Return list of dicts for each GPU: {'id', 'used', 'total', 'used_frac'}.\n       If NVML unavailable, returns empty list.\"\"\"\n    out = []\n    if not _NVML_AVAILABLE:\n        return out\n    device_count = pynvml.nvmlDeviceGetCount()\n    for i in range(device_count):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n        used = float(meminfo.used)\n        total = float(meminfo.total)\n        out.append({\"id\": i, \"used\": used, \"total\": total, \"used_frac\": used / total if total > 0 else 0.0})\n    return out\n\ndef any_vram_exceeds(frac_thresh: float) -> bool:\n    for g in gpu_memory_info():\n        if g[\"used_frac\"] >= frac_thresh:\n            return True\n    return False\n\ndef guard_memory(max_ram_frac: float, max_vram_frac: float) -> Tuple[bool,str]:\n    \"\"\"Return (ok, message). ok==False means we've reached thresholds and should stop.\"\"\"\n    ramf = host_ram_usage_frac()\n    if ramf >= max_ram_frac:\n        return False, f\"Host RAM fraction {ramf:.3f} >= max {max_ram_frac}\"\n    if _NVML_AVAILABLE:\n        for g in gpu_memory_info():\n            if g[\"used_frac\"] >= max_vram_frac:\n                return False, f\"GPU {g['id']} VRAM fraction {g['used_frac']:.3f} >= max {max_vram_frac}\"\n    return True, \"OK\"\n\ndef print_system_summary():\n    log.info(\"Torch CUDA available: %s\", torch.cuda.is_available())\n    log.info(\"Torch CUDA devices: %d\", torch.cuda.device_count())\n    log.info(\"Host RAM used fraction: %.3f\", host_ram_usage_frac())\n    if _NVML_AVAILABLE:\n        for g in gpu_memory_info():\n            log.info(\"GPU %d VRAM used %.3f / %.3f (frac %.3f)\", g['id'], g['used']/1e9, g['total']/1e9, g['used_frac'])\n    else:\n        log.info(\"NVML not available — GPU usage not shown.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 — Core helpers (col detection, to_frame) with added defensive checks and comments\n# -----------------------------------------------------------------------------\nimport re\n_int_re = re.compile(r'(-?\\d+(\\.\\d+)?)')\n\ndef find_col(df: pd.DataFrame, keys: List[str]) -> Optional[str]:\n    keys = [k.lower() for k in keys]\n    for c in df.columns:\n        cl = c.lower()\n        for k in keys:\n            if k in cl:\n                return c\n    return None\n\ndef to_frame(x: Any, fps: int = CONFIG[\"FPS\"]) -> Optional[int]:\n    \"\"\"Convert annotation value to frame index. Supports integers, seconds, timecodes like MM:SS or HH:MM:SS.\"\"\"\n    if pd.isnull(x):\n        return None\n    if isinstance(x, (int, np.integer)):\n        return int(x)\n    s = str(x).strip()\n    if ':' in s:\n        try:\n            parts = [float(p) for p in s.split(':')]\n            if len(parts) == 3:\n                secs = parts[0]*3600 + parts[1]*60 + parts[2]\n            elif len(parts) == 2:\n                secs = parts[0]*60 + parts[1]\n            else:\n                secs = parts[0]\n            return int(round(secs * fps))\n        except Exception:\n            return None\n    m = _int_re.search(s)\n    if m:\n        try:\n            xf = float(m.group(1))\n        except:\n            return None\n        # Heuristic: numbers <=20 are seconds\n        if xf <= 20:\n            return int(round(xf * fps))\n        return int(round(xf))\n    return None\n\ndef normalize_phase_map(phases_df: pd.DataFrame) -> Dict[str, int]:\n    mapping = {}\n    if phases_df is None or phases_df.shape[0] == 0:\n        return mapping\n    cols = [c.lower() for c in phases_df.columns]\n    if 'phase_id' in cols and 'phase_name' in cols:\n        id_col = phases_df.columns[cols.index('phase_id')]\n        name_col = phases_df.columns[cols.index('phase_name')]\n        for _, r in phases_df.iterrows():\n            mapping[str(r[name_col])] = int(r[id_col])\n        return mapping\n    if phases_df.shape[1] == 2:\n        id_col, name_col = phases_df.columns[0], phases_df.columns[1]\n        for _, r in phases_df.iterrows():\n            mapping[str(r[name_col])] = int(r[id_col])\n        return mapping\n    # fallback: last column as names\n    names = phases_df.iloc[:, -1].astype(str).tolist()\n    for i, n in enumerate(names):\n        mapping[n] = i\n    return mapping\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 — parse_annotations (step 1) with clear comments and safety checks\n# -----------------------------------------------------------------------------\ndef parse_annotations(base_dir: str = CONFIG[\"BASE_DIR\"], out_dir: Path = WORK_DIR, fps: int = CONFIG[\"FPS\"], sample_videos: Optional[int] = CONFIG[\"SAMPLE_VIDEOS\"]):\n    \"\"\"\n    Reads annotations.csv and phases.csv (if present), writes:\n      - segments_filled.csv\n      - labels_npy/<video_id>.npy  (per-frame labels)\n      - phase_map.csv\n      - video_file_map.csv (if videos folder exists)\n    This is robust to many annotation formats: explicit start/end, per-frame lists, MM:SS timecodes.\n    \"\"\"\n    base_dir = Path(base_dir)\n    ann_path = base_dir / \"annotations.csv\"\n    phases_path = base_dir / \"phases.csv\"\n    if not ann_path.exists():\n        raise FileNotFoundError(f\"{ann_path} missing\")\n    ann = pd.read_csv(ann_path)\n    log.info(\"Loaded annotations.csv (%d rows)\", len(ann))\n\n    # load phase map if available\n    phase_map = {}\n    if phases_path.exists():\n        try:\n            p_df = pd.read_csv(phases_path)\n            phase_map = normalize_phase_map(p_df)\n            log.info(\"Loaded phase_map (%d entries)\", len(phase_map))\n        except Exception as e:\n            log.warning(\"Failed to read phases.csv: %s\", e)\n\n    col_video = find_col(ann, ['video','video_id','videoid','vid','file','filename']) or ann.columns[0]\n    col_phase = find_col(ann, ['phase','label','phase_id','phaseid','class','action','phase_name'])\n    col_start = find_col(ann, ['start_frame','startframe','start_f','start'])\n    col_end = find_col(ann, ['end_frame','endframe','end_f','end'])\n    col_frame = find_col(ann, ['frame','frame_id','frameid'])\n    col_start_time = find_col(ann, ['start_time','startsec','start_s','startseconds'])\n    col_end_time = find_col(ann, ['end_time','endsec','end_s','endseconds'])\n    log.debug(\"Columns detected: video=%s phase=%s start=%s end=%s frame=%s\", col_video, col_phase, col_start, col_end, col_frame)\n\n    segments = []\n    if col_start is not None and col_end is not None and col_start in ann.columns and col_end in ann.columns:\n        log.info(\"Parsing explicit start/end columns.\")\n        for idx, row in ann.iterrows():\n            vid = str(row[col_video]) if col_video in ann.columns else str(idx)\n            sfrm = to_frame(row[col_start], fps=fps)\n            efrm = to_frame(row[col_end], fps=fps)\n            if sfrm is None or efrm is None:\n                continue\n            if col_phase and col_phase in ann.columns and not pd.isnull(row[col_phase]):\n                raw = row[col_phase]\n                key = str(raw)\n                if key in phase_map:\n                    pid = int(phase_map[key])\n                else:\n                    try:\n                        pid = int(float(raw))\n                    except:\n                        pid = len(phase_map)\n                        phase_map[key] = pid\n            else:\n                pid = 0\n            if efrm < sfrm:\n                sfrm, efrm = efrm, sfrm\n            segments.append((vid, int(sfrm), int(efrm), int(pid)))\n    elif col_frame is not None and col_frame in ann.columns:\n        log.info(\"Parsing compound/per-frame column: %s\", col_frame)\n        def parse_cell(cell):\n            if pd.isnull(cell):\n                return []\n            if isinstance(cell, (int, np.integer)):\n                v = int(cell)\n                return [(v, v, None)]\n            s = str(cell).strip()\n            s2 = s.replace(',', ';').replace('|', ';')\n            toks = [t.strip() for t in re.split(r'[;]+', s2) if t.strip()]\n            out = []\n            if len(toks) >= 3 and len(toks) % 3 == 0:\n                for i in range(0, len(toks), 3):\n                    a, b, c = toks[i], toks[i+1], toks[i+2]\n                    a_f, b_f = to_frame(a, fps), to_frame(b, fps)\n                    if a_f is None or b_f is None:\n                        continue\n                    out.append((a_f, b_f, c))\n                if out:\n                    return out\n            if len(toks) == 2:\n                a_f, b_f = to_frame(toks[0], fps), to_frame(toks[1], fps)\n                if a_f is not None and b_f is not None:\n                    return [(a_f, b_f, None)]\n            m = re.match(r'^\\s*(\\S+)\\s*[-–:]\\s*(\\S+)\\s*$', s)\n            if m:\n                a_f, b_f = to_frame(m.group(1), fps), to_frame(m.group(2), fps)\n                if a_f is not None and b_f is not None:\n                    return [(a_f, b_f, None)]\n            parts = s.split()\n            if len(parts) >= 2:\n                a_f, b_f = to_frame(parts[0], fps), to_frame(parts[1], fps)\n                if a_f is not None and b_f is not None:\n                    return [(a_f, b_f, None)]\n            single = to_frame(s, fps)\n            if single is not None:\n                return [(single, single, None)]\n            return []\n        for idx, row in ann.iterrows():\n            vid = str(row[col_video]) if col_video in ann.columns else str(idx)\n            parsed = parse_cell(row[col_frame])\n            if not parsed:\n                continue\n            for a, b, c in parsed:\n                pval = c if (c is not None and str(c).lower() not in ['nan','none','']) else (row[col_phase] if col_phase and col_phase in ann.columns else None)\n                if pval is None or (isinstance(pval, float) and np.isnan(pval)):\n                    pid = 0\n                else:\n                    key = str(pval)\n                    if key in phase_map:\n                        pid = int(phase_map[key])\n                    else:\n                        try:\n                            pid = int(float(key))\n                        except:\n                            pid = len(phase_map)\n                            phase_map[key] = pid\n                sfrm, efrm = int(a), int(b)\n                if efrm < sfrm:\n                    sfrm, efrm = efrm, sfrm\n                segments.append((vid, sfrm, efrm, pid))\n    else:\n        raise ValueError(\"No suitable start/end/frame column found in annotations.csv\")\n\n    seg_df = pd.DataFrame(segments, columns=['video_id','start_frame','end_frame','phase_id'])\n    if seg_df.empty:\n        raise RuntimeError(\"No segments parsed from annotations.csv\")\n    seg_df = seg_df[(seg_df['start_frame'] >= 0) & (seg_df['end_frame'] >= 0)].copy()\n    seg_df = seg_df.sort_values(['video_id','start_frame','end_frame']).reset_index(drop=True)\n\n    # fix 1-based indexing -> convert to 0-based\n    min_start = int(seg_df['start_frame'].min())\n    if min_start >= 1:\n        log.info(\"Detected probable 1-based indexing (min start_frame=%d) -> converting to 0-based\", min_start)\n        seg_df['start_frame'] -= 1\n        seg_df['end_frame'] -= 1\n        seg_df = seg_df[seg_df['end_frame'] >= seg_df['start_frame']].reset_index(drop=True)\n\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    seg_df.to_csv(out_dir / \"segments_filled.csv\", index=False)\n    log.info(\"Saved segments_filled.csv (%d segments, %d videos)\", len(seg_df), seg_df['video_id'].nunique())\n\n    # build per-video label arrays and save\n    LABELS_DIR = out_dir / \"labels_npy\"\n    LABELS_DIR.mkdir(parents=True, exist_ok=True)\n    for vid, g in seg_df.groupby('video_id'):\n        max_frame = int(g['end_frame'].max()) + 1\n        labels = np.full((max_frame,), -1, dtype=np.int32)\n        for _, r in g.iterrows():\n            s, e = int(r['start_frame']), int(r['end_frame'])\n            if s < 0: s = 0\n            if e < s: continue\n            labels[s:e+1] = int(r['phase_id'])\n        np.save(LABELS_DIR / f\"{vid}.npy\", labels, allow_pickle=False)\n    log.info(\"Saved label arrays to %s\", LABELS_DIR)\n\n    # write phase_map\n    phase_map_items = sorted(phase_map.items(), key=lambda x: x[1])\n    pd.DataFrame(phase_map_items, columns=['phase_name','phase_id']).to_csv(out_dir / \"phase_map.csv\", index=False)\n\n    # optional: write video_file_map if videos folder exists\n    vids_folder = Path(base_dir) / \"videos\"\n    if vids_folder.exists():\n        vm = {}\n        for p in vids_folder.iterdir():\n            if p.is_file():\n                vm[p.stem] = str(p)\n        if vm:\n            pd.DataFrame(list(vm.items()), columns=['video_id','video_file']).to_csv(out_dir / \"video_file_map.csv\", index=False)\n            log.info(\"Saved video_file_map.csv\")\n\n    return seg_df, phase_map\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 — build_backbone + preprocessing with multi-GPU / amp support\n# -----------------------------------------------------------------------------\nfrom torchvision import models\nimport torch.nn.functional as F\n\ndef build_backbone(backbone_name: str = CONFIG['FEATURE_BACKBONE'], pretrained: bool = CONFIG['PRETRAINED_BACKBONE'], device_ids: Optional[List[int]] = CONFIG['GPU_DEVICE_IDS']):\n    \"\"\"\n    Build a ResNet backbone, strip classifier, and return (model, feat_dim).\n    If device_ids has multiple entries, we return a DataParallel-wrapped model (note: DataParallel duplicates model across gpus).\n    \"\"\"\n    name = backbone_name.lower()\n    if name == 'resnet50':\n        base = models.resnet50(pretrained=pretrained)\n        feat_dim = 2048\n    elif name == 'resnet18':\n        base = models.resnet18(pretrained=pretrained)\n        feat_dim = 512\n    else:\n        raise ValueError(\"Unsupported backbone: \" + backbone_name)\n\n    backbone = nn.Sequential(*list(base.children())[:-1])  # remove classifier & avgpool left as global pool\n    backbone.eval()\n\n    # Device placement: prefer multi-gpu if requested and available\n    if torch.cuda.is_available() and (device_ids is None or len(device_ids) == 0):\n        device_ids = list(range(torch.cuda.device_count()))\n    if torch.cuda.is_available() and device_ids:\n        # send model to first GPU and wrap in DataParallel (simple path)\n        backbone = backbone.to(f\"cuda:{device_ids[0]}\")\n        if len(device_ids) > 1:\n            backbone = nn.DataParallel(backbone, device_ids=device_ids)\n    else:\n        backbone = backbone.to(\"cpu\")\n\n    # Minor perf tweaks\n    torch.backends.cudnn.benchmark = True\n    return backbone, feat_dim\n\ndef preprocess_frame_cv2(frame: np.ndarray, img_size: int = CONFIG['IMG_SIZE']):\n    \"\"\"Convert BGR cv2 frame -> normalized CHW float32 numpy array.\"\"\"\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    h, w = frame.shape[:2]\n    if (h, w) != (img_size, img_size):\n        frame = cv2.resize(frame, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n    arr = frame.astype(np.float32) / 255.0\n    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n    arr = (arr - mean) / std\n    arr = arr.transpose(2, 0, 1)   # HWC -> CHW\n    return arr\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 — extract_features_streaming (updated: uses torch.amp.autocast safely)\n# -----------------------------------------------------------------------------\nfrom contextlib import nullcontext\n\ndef amp_context_for_device(device: str):\n    \"\"\"\n    Return a callable that yields a context manager for mixed precision.\n    Usage:\n        amp_ctx = amp_context_for_device(device_str)\n        with amp_ctx():\n            ...\n    On CUDA devices returns torch.amp.autocast(device_type='cuda', enabled=True).\n    On CPU returns a no-op context (nullcontext).\n    \"\"\"\n    if torch.cuda.is_available() and device and str(device).startswith(\"cuda\"):\n        return lambda: torch.amp.autocast(device_type='cuda', enabled=True)\n    else:\n        return lambda: nullcontext()\n\ndef extract_features_streaming(work_dir: Path = WORK_DIR, base_dir: str = CONFIG['BASE_DIR'], sample_videos: Optional[int] = CONFIG['SAMPLE_VIDEOS']):\n    \"\"\"\n    Streaming feature extraction:\n     - Reads labels_npy/*.npy to find fragments to produce.\n     - For each base video, creates features_by_video/<base_vid>.npy (memmap if frame count known),\n       then slices that into per-fragment files saved in features/<stem>.npy.\n\n    Memory techniques and robustness:\n     - Uses memmap for full-video features when frame count known.\n     - Writes in small flushes and calls torch.cuda.empty_cache() after freeing tensors.\n     - Uses torch.amp.autocast (via amp_context_for_device) for mixed precision on CUDA.\n     - Adapts batch size downward on OOM; stops if memory thresholds exceeded.\n    \"\"\"\n    work_dir = Path(work_dir)\n    features_dir = work_dir / \"features\"\n    features_by_video_dir = work_dir / \"features_by_video\"\n    features_dir.mkdir(parents=True, exist_ok=True)\n    features_by_video_dir.mkdir(parents=True, exist_ok=True)\n\n    # Build video file map if present\n    video_file_map = {}\n    vm_path = work_dir / \"video_file_map.csv\"\n    if vm_path.exists():\n        vm_df = pd.read_csv(vm_path)\n        for _, r in vm_df.iterrows():\n            video_file_map[str(r['video_id'])] = str(r['video_file'])\n    videos_folder = Path(base_dir) / \"videos\"\n    video_files = {}\n    if videos_folder.exists():\n        for p in videos_folder.iterdir():\n            if p.is_file():\n                video_files[p.stem] = str(p)\n\n    # collect label stems and group by base video id (token before ';' if present)\n    label_stems = sorted([p.stem for p in (work_dir/'labels_npy').glob('*.npy')])\n    if sample_videos is not None:\n        label_stems = label_stems[:sample_videos]\n    groups = {}\n    for stem in label_stems:\n        base = re.split(r'[;,\\|:]+', stem)[0].strip()\n        groups.setdefault(base, []).append(stem)\n\n    log.info(\"Found %d label fragments grouped into %d base videos\", len(label_stems), len(groups))\n\n    # Build backbone\n    # autodetect device ids for multi-gpu\n    if CONFIG['GPU_DEVICE_IDS'] is None:\n        device_ids = list(range(torch.cuda.device_count())) if torch.cuda.is_available() else []\n    else:\n        device_ids = CONFIG['GPU_DEVICE_IDS']\n    backbone, feat_dim = build_backbone(CONFIG['FEATURE_BACKBONE'], CONFIG['PRETRAINED_BACKBONE'], device_ids=device_ids)\n\n    # initial batch_frame (may reduce on OOM)\n    batch_frame = int(CONFIG['BATCH_FRAME'])\n\n    # For each base video, create features_by_video file then slice fragments\n    for base_vid, stems in groups.items():\n        log.info(\"Processing base video: %s | fragments: %d\", base_vid, len(stems))\n        # memory guard before starting a new video\n        ok, msg = guard_memory(CONFIG['MAX_RAM_FRAC'], CONFIG['MAX_VRAM_FRAC'])\n        if not ok:\n            log.warning(\"Aborting processing: memory guard tripped before starting video %s: %s\", base_vid, msg)\n            break\n\n        # find video path\n        video_path = None\n        if base_vid in video_file_map:\n            candidate = Path(video_file_map[base_vid])\n            if not candidate.is_absolute():\n                candidate = Path(base_dir) / \"videos\" / candidate\n            if candidate.exists():\n                video_path = str(candidate)\n        if video_path is None and base_vid in video_files:\n            video_path = video_files[base_vid]\n        if video_path is None:\n            # try fuzzy\n            for key, path in video_files.items():\n                if key.startswith(base_vid) or base_vid in key:\n                    video_path = path\n                    break\n        if video_path is None:\n            log.warning(\"No source video for %s — skipping\", base_vid)\n            continue\n\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            log.warning(\"Failed to open %s — skipping\", video_path)\n            continue\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0 else None\n        full_feat_path = features_by_video_dir / f\"{base_vid}.npy\"\n\n        # If precomputed full features exist, slice them and continue\n        if full_feat_path.exists():\n            log.info(\"Full-video features already exist for %s — slicing fragments\", base_vid)\n            try:\n                full_feats = np.load(full_feat_path, mmap_mode='r')\n            except Exception as e:\n                log.warning(\"Failed to mmap existing features file: %s (will recompute). Error: %s\", full_feat_path, e)\n                full_feat_path.unlink(missing_ok=True)\n            else:\n                for stem in stems:\n                    outp = features_dir / f\"{stem}.npy\"\n                    if outp.exists():\n                        continue\n                    lab_path = work_dir / 'labels_npy' / f\"{stem}.npy\"\n                    if not lab_path.exists():\n                        log.warning(\"Label missing for %s — skipping\", stem)\n                        continue\n                    lab = np.load(lab_path)\n                    L = lab.shape[0]\n                    if full_feats.shape[0] >= L:\n                        frag_feats = full_feats[:L]\n                    else:\n                        pad_n = L - full_feats.shape[0]\n                        pad_row = full_feats[-1][None].repeat(pad_n, axis=0)\n                        frag_feats = np.concatenate([full_feats, pad_row], axis=0)\n                    np.save(outp, frag_feats, allow_pickle=False)\n                cap.release()\n                continue\n\n        # create memmap if we know frame count\n        try:\n            # Determine a device string for AMP helper (could be \"cuda:0\" or \"cpu\")\n            try:\n                _device = next(backbone.parameters()).device\n            except StopIteration:\n                # unexpected: backbone has no parameters; default to cpu\n                _device = torch.device(\"cpu\")\n            device_str = str(_device)\n\n            if total_frames and total_frames > 0:\n                log.info(\"Creating memmap for %s frames=%d feat_dim=%d\", base_vid, total_frames, feat_dim)\n                mm = np.lib.format.open_memmap(str(full_feat_path), mode='w+', dtype='float32', shape=(total_frames, feat_dim))\n                write_pos = 0\n                batch = []\n                with torch.no_grad():\n                    amp_ctx = amp_context_for_device(device_str)\n                    while True:\n                        ret, frame = cap.read()\n                        if not ret:\n                            break\n                        arr = preprocess_frame_cv2(frame, img_size=CONFIG['IMG_SIZE'])\n                        batch.append(arr)\n                        if len(batch) >= batch_frame:\n                            ok, msg = guard_memory(CONFIG['MAX_RAM_FRAC'], CONFIG['MAX_VRAM_FRAC'])\n                            if not ok:\n                                log.warning(\"Memory threshold reached during encoding: %s\", msg)\n                                raise RuntimeError(\"Memory threshold reached\")\n                            try:\n                                with amp_ctx():\n                                    tensor_batch = torch.from_numpy(np.stack(batch, axis=0)).to(next(backbone.parameters()).device)\n                                    out = backbone(tensor_batch).view(tensor_batch.size(0), -1).cpu().numpy()\n                                    mm[write_pos:write_pos+out.shape[0]] = out.astype('float32')\n                                    write_pos += out.shape[0]\n                                    batch = []\n                            except RuntimeError as e:\n                                # Handle CUDA OOM by stepping down batch size and retrying\n                                log.warning(\"RuntimeError during encoding (likely OOM): %s — reducing batch_frame and retrying\", e)\n                                torch.cuda.empty_cache()\n                                if batch_frame > CONFIG['MIN_BATCH_FRAME']:\n                                    batch_frame = max(CONFIG['MIN_BATCH_FRAME'], batch_frame // 2)\n                                    log.info(\"Reduced batch_frame to %d\", batch_frame)\n                                    time.sleep(0.5)\n                                    continue\n                                else:\n                                    raise\n                    # leftover batch\n                    if batch:\n                        with amp_ctx():\n                            tensor_batch = torch.from_numpy(np.stack(batch, axis=0)).to(next(backbone.parameters()).device)\n                            out = backbone(tensor_batch).view(tensor_batch.size(0), -1).cpu().numpy()\n                            mm[write_pos:write_pos+out.shape[0]] = out.astype('float32')\n                            write_pos += out.shape[0]\n                cap.release()\n                # truncate if fewer frames encoded than declared\n                if write_pos != mm.shape[0]:\n                    log.debug(\"Truncating memmap from %d -> %d\", mm.shape[0], write_pos)\n                    mm.flush()\n                    del mm\n                    arr = np.load(str(full_feat_path), mmap_mode='r')\n                    arr2 = arr[:write_pos]\n                    np.save(str(full_feat_path), arr2, allow_pickle=False)\n                else:\n                    mm.flush()\n                    del mm\n                log.info(\"Saved full-video features: %s\", full_feat_path)\n            else:\n                # Unknown frame count -> accumulate in chunks and save\n                feats_chunks = []\n                batch = []\n                with torch.no_grad():\n                    amp_ctx = amp_context_for_device(device_str)\n                    while True:\n                        ret, frame = cap.read()\n                        if not ret:\n                            break\n                        arr = preprocess_frame_cv2(frame, img_size=CONFIG['IMG_SIZE'])\n                        batch.append(arr)\n                        if len(batch) >= batch_frame:\n                            ok, msg = guard_memory(CONFIG['MAX_RAM_FRAC'], CONFIG['MAX_VRAM_FRAC'])\n                            if not ok:\n                                log.warning(\"Memory threshold reached during encoding: %s\", msg)\n                                raise RuntimeError(\"Memory threshold reached\")\n                            try:\n                                with amp_ctx():\n                                    tensor_batch = torch.from_numpy(np.stack(batch, axis=0)).to(next(backbone.parameters()).device)\n                                    out = backbone(tensor_batch).view(tensor_batch.size(0), -1).cpu().numpy()\n                                    feats_chunks.append(out.astype('float32'))\n                                    batch = []\n                            except RuntimeError as e:\n                                log.warning(\"OOM during encoding: %s\", e)\n                                torch.cuda.empty_cache()\n                                if batch_frame > CONFIG['MIN_BATCH_FRAME']:\n                                    batch_frame = max(CONFIG['MIN_BATCH_FRAME'], batch_frame // 2)\n                                    log.info(\"Reduced batch_frame to %d\", batch_frame)\n                                    time.sleep(0.5)\n                                    continue\n                                else:\n                                    raise\n                    if batch:\n                        with amp_ctx():\n                            tensor_batch = torch.from_numpy(np.stack(batch, axis=0)).to(next(backbone.parameters()).device)\n                            out = backbone(tensor_batch).view(tensor_batch.size(0), -1).cpu().numpy()\n                            feats_chunks.append(out.astype('float32'))\n                cap.release()\n                if len(feats_chunks) == 0:\n                    log.warning(\"No frames encoded for %s\", base_vid)\n                    continue\n                full_feats = np.concatenate(feats_chunks, axis=0)\n                np.save(full_feat_path, full_feats, allow_pickle=False)\n                log.info(\"Saved fallback full-video features: %s shape %s\", full_feat_path, full_feats.shape)\n                del feats_chunks, full_feats\n\n            # Slice and save per-fragment features using mmap load (minimal RAM)\n            try:\n                full_feats = np.load(full_feat_path, mmap_mode='r')\n            except Exception as e:\n                log.error(\"Failed to open saved full video features for slicing: %s (%s)\", full_feat_path, e)\n                continue\n            for stem in stems:\n                outp = features_dir / f\"{stem}.npy\"\n                if outp.exists():\n                    continue\n                lab_path = work_dir / 'labels_npy' / f\"{stem}.npy\"\n                if not lab_path.exists():\n                    log.warning(\"Label missing for fragment %s — skipping\", stem)\n                    continue\n                lab = np.load(lab_path)\n                L = lab.shape[0]\n                if full_feats.shape[0] >= L:\n                    frag_feats = full_feats[:L]\n                else:\n                    pad_n = L - full_feats.shape[0]\n                    pad_row = full_feats[-1][None].repeat(pad_n, axis=0)\n                    frag_feats = np.concatenate([full_feats, pad_row], axis=0)\n                np.save(outp, frag_feats, allow_pickle=False)\n            # Free and clear GPU cache\n            del full_feats\n            torch.cuda.empty_cache()\n            time.sleep(CONFIG['SAFE_SLEEP_SEC'])\n        except Exception as e:\n            log.error(\"Error processing %s: %s\", base_vid, e)\n            try:\n                cap.release()\n            except:\n                pass\n            # if memmap exists but a partial product was created, leave it for inspection\n            continue\n\n    log.info(\"Streaming feature extraction finished. Per-video features in %s and per-fragment features in %s\", features_by_video_dir, features_dir)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 — ClipDataset (mmap-backed) with careful mem usage & docstring\n# -----------------------------------------------------------------------------\nimport torch.utils.data as data\n\nclass ClipDataset(data.Dataset):\n    \"\"\"\n    Dataset that returns clips of length seq_len from memmapped per-video features.\n    Keeps labels for chosen videos in-memory (labels are small), but loads features lazily with mmap.\n    \"\"\"\n    def __init__(self, work_dir: Path = WORK_DIR, seq_len: int = CONFIG['SEQ_LEN'], sample_videos: Optional[int] = CONFIG['SAMPLE_VIDEOS'], seed: int = 42):\n        self.work_dir = Path(work_dir)\n        self.features_dir = self.work_dir / 'features'\n        self.labels_dir = self.work_dir / 'labels_npy'\n        self.seq_len = seq_len\n\n        label_stems = sorted([p.stem for p in self.labels_dir.glob('*.npy')])\n        feature_stems = set([p.stem for p in self.features_dir.glob('*.npy')])\n        avail = [s for s in label_stems if s in feature_stems]\n        if len(avail) == 0:\n            raise RuntimeError(\"No videos with both labels and features found. Run feature extraction first.\")\n\n        rng = np.random.RandomState(seed)\n        rng.shuffle(avail)\n        if sample_videos is not None:\n            avail = avail[:sample_videos]\n        self.videos = avail\n\n        # load labels fully (labels arrays are small relative to features)\n        self.video_labels = {v: np.load(self.labels_dir / f\"{v}.npy\", mmap_mode=None) for v in self.videos}\n\n        # build sliding-window index with stride = seq_len // 2\n        self.index = []\n        for v in self.videos:\n            n = len(self.video_labels[v])\n            if n < self.seq_len:\n                continue\n            stride = max(1, self.seq_len // 2)\n            for s in range(0, n - self.seq_len + 1, stride):\n                self.index.append((v, s))\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, idx):\n        vid, s = self.index[idx]\n        feat_path = self.features_dir / f\"{vid}.npy\"\n        if not feat_path.exists():\n            raise FileNotFoundError(f\"Missing features for {vid} at {feat_path}\")\n        feats = np.load(feat_path, mmap_mode='r')\n        clip_feats = feats[s:s+self.seq_len].astype('float32')\n        clip_lbls = self.video_labels[vid][s:s+self.seq_len].astype('int64')\n        return {'video_id': vid, 'start': s, 'feats': torch.from_numpy(clip_feats).float(), 'labels': torch.from_numpy(clip_lbls).long()}\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 — quick_train_example with amp and gradient accumulation (optional)\n# -----------------------------------------------------------------------------\ndef quick_train_example(work_dir: Path = WORK_DIR, seq_len: int = 100, batch_size: int = 2, epochs: int = 3, lr: float = 1e-3):\n    \"\"\"\n    Small training loop demonstrating safe GPU usage:\n      - uses AMP for mixed precision\n      - optional DataParallel if multiple GPUs detected\n      - uses gradient accumulation to control effective batch size\n    \"\"\"\n    ds = ClipDataset(work_dir=work_dir, seq_len=seq_len, sample_videos=CONFIG['SAMPLE_VIDEOS'])\n    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=lambda x: x)\n    # find input dim by peeking at one feature file\n    feat_sample_path = next((work_dir/'features').glob('*.npy'))\n    input_dim = np.load(feat_sample_path, mmap_mode='r').shape[1]\n    rnn = nn.GRU(input_dim, 64, batch_first=True)\n    head = nn.Linear(64, 10)  # change 10 -> num classes\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cuda\" and torch.cuda.device_count() > 1:\n        rnn = nn.DataParallel(rnn)\n        head = nn.DataParallel(head)\n    rnn.to(device); head.to(device)\n    opt = torch.optim.Adam(list(rnn.parameters()) + list(head.parameters()), lr=lr)\n    scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\n    for epoch in range(epochs):\n        log.info(\"Epoch %d/%d\", epoch+1, epochs)\n        for batch in dl:\n            # collate\n            feats = torch.stack([b['feats'] for b in batch], dim=0).to(device)\n            labels = torch.stack([b['labels'] for b in batch], dim=0).to(device)\n            with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n                out, _ = rnn(feats)\n                logits = head(out)  # B x T x C\n                loss = nn.functional.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=-1)\n            opt.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n            torch.cuda.empty_cache()\n        log.info(\"Epoch %d done (loss %.4f)\", epoch+1, float(loss.detach().cpu().item()))\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10 — Main usage snippet (run in __main__ or cells sequentially)\n# -----------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    print_system_summary()\n    # 1) parse annotations & create label arrays\n    seg_df, phase_map = parse_annotations(CONFIG['BASE_DIR'], WORK_DIR, CONFIG['FPS'], CONFIG['SAMPLE_VIDEOS'])\n    # 2) extract features streaming\n    extract_features_streaming(WORK_DIR, CONFIG['BASE_DIR'], CONFIG['SAMPLE_VIDEOS'])\n    # 3) dataset and optional quick train\n    # ds = ClipDataset(WORK_DIR)\n    # quick_train_example(work_dir=WORK_DIR, seq_len=CONFIG['SEQ_LEN'], batch_size=2, epochs=1)\n    print(\"All done for this run. Review logs above.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 — Upload output folder to Hugging Face (final step)\n# -----------------------------------------------------------------------------\n# Note: requires HF_TOKEN in environment. Uses hf_api uploaded earlier.\nfolder_to_upload = str(WORK_DIR)\nrepo_id = CONFIG['HF_REPO_ID']\nrepo_type = CONFIG['HF_REPO_TYPE']\n\n# Safety check: do not attempt upload if HF_TOKEN missing\nif not HF_TOKEN:\n    log.error(\"HF_TOKEN missing — cannot upload. Set HF_TOKEN environment variable and re-run.\")\nelse:\n    # upload_folder will fail if repo doesn't exist; you can create a dataset repo on HF or set repo_id appropriately.\n    log.info(\"Uploading folder %s to Hugging Face repo %s (type=%s). This may take time.\", folder_to_upload, repo_id, repo_type)\n    try:\n        hf_api.upload_folder(folder_path=folder_to_upload, repo_id=repo_id, repo_type=repo_type)\n        log.info(\"Upload finished (or the call returned). Check the HF repo to confirm.\")\n    except Exception as e:\n        log.error(\"Upload failed: %s\", e)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}